{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e69874",
   "metadata": {},
   "source": [
    "# Exploring Score DataFrame Sizes\n",
    "\n",
    "As we combine the output DataFrames from multiple scoring methods, it is useful to know the sizes of each dataframe and how much overlap they have with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7230d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from grn_inference.normalization import minmax_normalize_pandas\n",
    "\n",
    "project_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER\"\n",
    "input_dir = os.path.join(project_dir, \"input/DS011_mESC/DS011_mESC_sample1\")\n",
    "output_dir = os.path.join(project_dir, \"output/DS011_mESC/DS011_mESC_sample1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e64604",
   "metadata": {},
   "source": [
    "## Size of the Input Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa378019",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_processed_df = pd.read_parquet(\n",
    "    os.path.join(input_dir, \"DS011_mESC_ATAC_processed.parquet\"), \n",
    "    engine=\"pyarrow\"\n",
    "    )\n",
    "\n",
    "rna_processed_df = pd.read_parquet(\n",
    "    os.path.join(input_dir, \"DS011_mESC_RNA_processed.parquet\"), \n",
    "    engine=\"pyarrow\"\n",
    "    )\n",
    "\n",
    "print(\"scATAC-seq Dataset:\")\n",
    "print(f\"  - Cells: {atac_processed_df.shape[1]-1}\")\n",
    "print(f\"  - Peaks: {atac_processed_df.shape[0]}\")\n",
    "\n",
    "print(\"\\nscRNA-seq Dataset:\")\n",
    "print(f\"  - Cells: {rna_processed_df.shape[1]-1}\")\n",
    "print(f\"  - Genes: {rna_processed_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afcadbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc442f",
   "metadata": {},
   "source": [
    "## Peak to TG\n",
    "\n",
    "The peak to TG regulatory potential methods we use are:\n",
    "\n",
    "1) Cicero\n",
    "2) TSS Distance\n",
    "3) MIRA\n",
    "4) TG Expression / Peak Accessibility correlation\n",
    "\n",
    "First, let's look at the size of each methods output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_to_tg_score_summary(score_file_name, method_name) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(output_dir, score_file_name)\n",
    "    )\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\")\n",
    "    print(f\"  - Unique TGs: {df['target_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "method_files = {\n",
    "    \"Cicero\": \"cicero_peak_to_tg_scores.parquet\",\n",
    "    \"TSS Distance Score\": \"tss_distance_score.parquet\",\n",
    "    \"MIRA\": \"ds011_peak_to_gene_lite_rp_score_chrom_diff.parquet\",\n",
    "    \"Correlation\": \"peak_to_gene_correlation.parquet\"\n",
    "}\n",
    "\n",
    "method_dfs = {\n",
    "    name: peak_to_tg_score_summary(path, name)\n",
    "    for name, path in method_files.items()\n",
    "}\n",
    "\n",
    "method_peaks = {name: set(df[\"peak_id\"]) for name, df in method_dfs.items()}\n",
    "method_targets = {name: set(df[\"target_id\"]) for name, df in method_dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d80d2c",
   "metadata": {},
   "source": [
    "The TSS distance score DataFrame is much larger than the others. Let's look at how many of the peak to TG edges are shared between scoring methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_elements(method_dict, label):\n",
    "    shared_elements = set()\n",
    "    method_names = list(method_dict.keys())\n",
    "    print(f\"\\nShared {label}s:\")\n",
    "    for i, m1 in enumerate(method_names):\n",
    "        for m2 in method_names[i+1:]:\n",
    "            shared = method_dict[m1] & method_dict[m2]\n",
    "            print(f\"  - {m1} vs {m2}: {len(shared):,}\")\n",
    "            shared_elements.update(shared)\n",
    "    return shared_elements\n",
    "\n",
    "# Compute shared peaks and target genes\n",
    "shared_peaks = get_shared_elements(method_peaks, \"Peak\")\n",
    "shared_targets = get_shared_elements(method_targets, \"Target Gene\")\n",
    "\n",
    "print(f\"\\nPeaks in more than one method: {len(shared_peaks):,}\")\n",
    "print(f\"Target genes in more than one method: {len(shared_targets):,}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64db4a3",
   "metadata": {},
   "source": [
    "There is not a ton of overlap, but this reduces the size of the DataFrame. Now, lets subset each score DataFrame to only use edges that are in more than one scoring method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df_to_peaks_and_targets_in_more_than_one_df(df, method_name, shared_peaks, shared_targets) -> pd.DataFrame:\n",
    "    df = df[\n",
    "    (df[\"peak_id\"].isin(shared_peaks)) &\n",
    "    (df[\"target_id\"].isin(shared_targets))\n",
    "    ].dropna()\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\")\n",
    "    print(f\"  - Unique TGs: {df['target_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Filter each dataframe to only those peak-target pairs that are in multiple methods\n",
    "method_dfs_subset = {\n",
    "    name: subset_df_to_peaks_and_targets_in_more_than_one_df(df, name, shared_peaks, shared_targets)\n",
    "    for name, df in method_dfs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d47707",
   "metadata": {},
   "source": [
    "Once we have only edges found in more than one score DataFrame, we can merge the scoring methods together into a single tf to peak DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_scores_no_tss = pd.merge(\n",
    "    pd.merge(method_dfs_subset[\"Cicero\"], method_dfs_subset[\"MIRA\"], on=[\"peak_id\", \"target_id\"], how=\"outer\"),\n",
    "    method_dfs_subset[\"Correlation\"], on=[\"peak_id\", \"target_id\"], how=\"inner\"\n",
    ")\n",
    "\n",
    "peak_to_tg = pd.merge(\n",
    "    merge_scores_no_tss, \n",
    "    method_dfs_subset[\"TSS Distance Score\"], \n",
    "    on=[\"peak_id\", \"target_id\"], \n",
    "    how=\"inner\"\n",
    "    )\n",
    "peak_to_tg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a414ab",
   "metadata": {},
   "source": [
    "Let's look at how sparse the dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_edges = len(peak_to_tg[[\"peak_id\", \"target_id\"]])\n",
    "score_cols = {\n",
    "    \"Cicero Score\": \"cicero_score\",\n",
    "    \"TSS Distance Score\": \"TSS_dist_score\",\n",
    "    \"LITE Score\": \"LITE_score\",\n",
    "    \"NITE Score\": \"NITE_score\",\n",
    "    \"MIRA Chromatin Differential Score\": \"chromatin_differential\",\n",
    "    \"Correlation Score\": \"correlation\"\n",
    "}\n",
    "\n",
    "print(f\"Total Edges: {total_edges:,}\")\n",
    "for label, col in score_cols.items():\n",
    "    score_count = peak_to_tg[col].notna().sum()\n",
    "    print(f\"  - {label}: {score_count:,} ({score_count / total_edges:.2%} of total scores)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373f404",
   "metadata": {},
   "source": [
    "The merged dataset isn't too sparse if we outer merge the method score DataFrames, but use an inner merge for the correlation and TSS distance scores. The number of peak to TG edges has dropped dramatically, which may or may not be a good thing. We can add a column called `\"support_count\"` which counts the number methods with non-nan values across rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b374869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many scores are non-null for each edge\n",
    "score_support_counts = peak_to_tg[score_cols.values()].notna().sum(axis=1)\n",
    "\n",
    "# Select edges supported by 2 or more methods\n",
    "for i, _ in enumerate(score_cols.values()):\n",
    "    if i > 0:\n",
    "        multi_supported_edges = peak_to_tg[score_support_counts >= i]\n",
    "\n",
    "        print(f\"Edges supported by â‰¥{i} methods: {len(multi_supported_edges):,} out of {len(peak_to_tg):,}\")\n",
    "peak_to_tg[\"support_count\"] = score_support_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750eb66",
   "metadata": {},
   "source": [
    "Now that we have a DataFrame containing all of the peak to TG scores for at least 2 methods, we can add in the **minmax normalized mean peak accessibility and mean TG expression** for each edge. We use an inner join here to only keep rows that have genes and peaks in the expression and accesibility data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc718024",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_peak_expr_norm = minmax_normalize_pandas(\n",
    "    atac_processed_df.set_index(\"peak_id\").mean(axis=1).rename(\"mean_peak_expr\").to_frame(),\n",
    "    [\"mean_peak_expr\"]\n",
    ").reset_index()\n",
    "\n",
    "mean_gene_expr_norm = minmax_normalize_pandas(\n",
    "    rna_processed_df.set_index(\"gene_id\").mean(axis=1).rename(\"mean_gene_expr\").to_frame(),\n",
    "    [\"mean_gene_expr\"]\n",
    ").reset_index()\n",
    "\n",
    "peak_to_tg_mean_acc = pd.merge(peak_to_tg, mean_peak_expr_norm, on=\"peak_id\", how=\"inner\")\n",
    "full_peak_to_tg = pd.merge(\n",
    "    peak_to_tg_mean_acc, mean_gene_expr_norm, left_on=\"target_id\", \n",
    "    right_on=\"gene_id\", how=\"inner\"\n",
    "    ).drop(columns=\"gene_id\").rename(columns={\"mean_gene_expr\":\"mean_tg_expr\"})\n",
    "full_peak_to_tg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74063ed8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3512676",
   "metadata": {},
   "source": [
    "## TF to Peak\n",
    "\n",
    "Now that we have the peak to TG scores together, we need to combine the TF to peak binding potential scores. We have two methods that calculate TF to peak scores:\n",
    "\n",
    "1) Homer\n",
    "2) Sliding Window\n",
    "\n",
    "Let's start by loading these score DataFrames in and take a look at their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5587b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_to_peak_score_summary(score_file_name, method_name) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(output_dir, score_file_name)\n",
    "    )\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique TFs: {df['source_id'].nunique():,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "method_files = {\n",
    "    \"Homer\": \"homer_tf_to_peak.parquet\",\n",
    "    \"Sliding Window\": \"sliding_window_tf_to_peak_score.parquet\"\n",
    "}\n",
    "\n",
    "method_dfs = {\n",
    "    name: tf_to_peak_score_summary(path, name)\n",
    "    for name, path in method_files.items()\n",
    "}\n",
    "\n",
    "method_TFs = {name: set(df[\"source_id\"]) for name, df in method_dfs.items()}\n",
    "method_peaks = {name: set(df[\"peak_id\"]) for name, df in method_dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5265a",
   "metadata": {},
   "source": [
    "We will merge only on shared peaks and TFs between the two due to the large number of edges. We will also merge in the mean gene expression for the TFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_df = method_dfs[\"Sliding Window\"]\n",
    "homer_df = method_dfs[\"Homer\"]\n",
    "\n",
    "tf_to_peak_df = pd.merge(\n",
    "    pd.merge(sliding_window_df, homer_df, on=[\"peak_id\", \"source_id\"], how=\"inner\"),\n",
    "    mean_gene_expr_norm, left_on=\"source_id\", right_on=\"gene_id\", how=\"inner\"\n",
    "    ).drop(columns=\"gene_id\").rename(columns={\"mean_gene_expr\":\"mean_tf_expr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95985ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df = pd.merge(full_peak_to_tg, tf_to_peak_df, on=\"peak_id\", how=\"right\").dropna(subset=[\"source_id\", \"peak_id\", \"target_id\"])\n",
    "tf_to_tg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9756477",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df = tf_to_tg_df[[\n",
    "    'source_id', 'peak_id', 'target_id', \n",
    "    'mean_tf_expr', 'mean_peak_expr', 'mean_tg_expr',\n",
    "    'cicero_score', 'TSS_dist_score', 'correlation',\n",
    "    'LITE_score', 'NITE_score', 'chromatin_differential', \n",
    "    'sliding_window_score', 'homer_binding_score', 'support_count'\n",
    "    ]]\n",
    "tf_to_tg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df = minmax_normalize_pandas(tf_to_tg_df, [\"NITE_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_to_tg_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe to get it in the same format as combine_dataframes.py\n",
    "score_cols = [\n",
    "    'mean_tf_expr', 'mean_peak_expr', 'mean_tg_expr',\n",
    "    'cicero_score', 'TSS_dist_score', 'correlation',\n",
    "    'LITE_score', 'NITE_score', 'chromatin_differential', \n",
    "    'sliding_window_score', 'homer_binding_score', 'support_count'\n",
    "    ]\n",
    "\n",
    "melted_df = tf_to_tg_df.melt(\n",
    "    id_vars=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "    value_vars=score_cols,\n",
    "    var_name=\"score_type\",\n",
    "    value_name=\"score_value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eef0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df.to_parquet(os.path.join(output_dir, \"inferred_grns\", \"inferred_score_df.parquet\"), engine=\"pyarrow\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b4cff",
   "metadata": {},
   "source": [
    "## Adding STRING Scores to the Combined Score DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d095989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grn_inference.normalization import clip_and_normalize_log1p_pandas, minmax_normalize_pandas\n",
    "\n",
    "def add_string_db_scores(inferred_net_df):\n",
    "    string_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/data/string_database/mm10\"\n",
    "    \n",
    "    # Load STRING protein info and links (small)\n",
    "    print(\"  - Reading STRING protein info\")\n",
    "    protein_info_df = pd.read_csv(f\"{string_dir}/protein_info.txt\", sep=\"\\t\")\n",
    "    \n",
    "    print(\"  - Reading STRING protein links detailed\")\n",
    "    protein_links_df = pd.read_csv(f\"{string_dir}/protein_links_detailed.txt\", sep=\" \")\n",
    "\n",
    "    print(\"  - Mapping STRING protein IDs to preferred names\")\n",
    "    id_to_name = protein_info_df.set_index(\"#string_protein_id\")[\"preferred_name\"].to_dict()\n",
    "    protein_links_df[\"protein1\"] = protein_links_df[\"protein1\"].map(id_to_name)\n",
    "    protein_links_df[\"protein2\"] = protein_links_df[\"protein2\"].map(id_to_name)\n",
    "    \n",
    "    inferred_net_df[\"source_id\"] = inferred_net_df[\"source_id\"].str.strip().str.upper()\n",
    "    inferred_net_df[\"target_id\"] = inferred_net_df[\"target_id\"].str.strip().str.upper()\n",
    "    protein_links_df[\"protein1\"] = protein_links_df[\"protein1\"].str.strip().str.upper()\n",
    "    protein_links_df[\"protein2\"] = protein_links_df[\"protein2\"].str.strip().str.upper()\n",
    "\n",
    "    id_to_name = {\n",
    "        k: v.strip().upper() for k, v in \n",
    "        protein_info_df.set_index(\"#string_protein_id\")[\"preferred_name\"].to_dict().items()\n",
    "    }\n",
    "\n",
    "    # Bi-directional match\n",
    "    tf_tg_pairs = set(zip(inferred_net_df[\"source_id\"], inferred_net_df[\"target_id\"]))\n",
    "    tf_tg_pairs |= set((tg, tf) for tf, tg in tf_tg_pairs)  # include reverse\n",
    "    \n",
    "    print(f\"  - Filtering STRING links to match {len(tf_tg_pairs):,} TFâ€“TG pairs\")\n",
    "    filtered_links_df = protein_links_df[\n",
    "        protein_links_df[[\"protein1\", \"protein2\"]]\n",
    "        .apply(tuple, axis=1)\n",
    "        .isin(tf_tg_pairs)\n",
    "    ]\n",
    "\n",
    "    # Rename columns and normalize\n",
    "    filtered_links_df = filtered_links_df.rename(columns={\n",
    "        \"experimental\": \"string_experimental_score\",\n",
    "        \"textmining\": \"string_textmining_score\",\n",
    "        \"combined_score\": \"string_combined_score\"\n",
    "    })[[\"protein1\", \"protein2\", \"string_experimental_score\", \"string_textmining_score\", \"string_combined_score\"]]\n",
    "\n",
    "    # filtered_links_df = clip_and_normalize_log1p_pandas(\n",
    "    #     df=filtered_links_df,\n",
    "    #     score_cols=[\"string_experimental_score\", \"string_textmining_score\", \"string_combined_score\"],\n",
    "    #     quantiles=(0.05, 0.95),\n",
    "    #     apply_log1p=True,\n",
    "    #     sample_frac=0.1  # optional: for speed\n",
    "    # )\n",
    "\n",
    "    filtered_links_df = minmax_normalize_pandas(\n",
    "        df=filtered_links_df,\n",
    "        score_cols=[\"string_experimental_score\", \"string_textmining_score\", \"string_combined_score\"],\n",
    "    )\n",
    "\n",
    "    print(\"  - Merging normalized STRING scores into inferred network\")\n",
    "    merged_dd = inferred_net_df.merge(\n",
    "        filtered_links_df,\n",
    "        left_on=[\"source_id\", \"target_id\"],\n",
    "        right_on=[\"protein1\", \"protein2\"],\n",
    "        how=\"left\"\n",
    "    ).drop(columns=[\"protein1\", \"protein2\"])\n",
    "\n",
    "    print(\"  Done!\")\n",
    "    return merged_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47900039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inferred_network(inferred_network_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and pivot a melted sparse inferred network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inferred_network_file : str\n",
    "        Path to a parquet file containing a \"melted\" network with columns\n",
    "        ``source_id``, ``peak_id``, ``target_id``, ``score_type`` and\n",
    "        ``score_value``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dask.dataframe.DataFrame\n",
    "        Network in wide format where score types form columns and the index is\n",
    "        ``(source_id, peak_id, target_id)``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The returned dataframe has a single partition which is usually sufficient\n",
    "    for moderate sized networks.  Adjust ``npartitions`` if necessary.\n",
    "    \"\"\"\n",
    "    print(f\"Loading melted sparse network from: {inferred_network_file}\")\n",
    "    melted_ddf = pd.read_parquet(inferred_network_file, engine=\"pyarrow\")\n",
    "\n",
    "    # Standardize IDs\n",
    "    melted_ddf[\"source_id\"] = melted_ddf[\"source_id\"].str.upper()\n",
    "    melted_ddf[\"target_id\"] = melted_ddf[\"target_id\"].str.upper()\n",
    "\n",
    "    # Aggregate scores\n",
    "    grouped_ddf = (\n",
    "        melted_ddf\n",
    "        .groupby([\"source_id\", \"peak_id\", \"target_id\", \"score_type\"])[\"score_value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot manually by converting to pandas (if dataset is small enough)\n",
    "    def pivot_partition(df):\n",
    "        return df.pivot_table(\n",
    "            index=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "            columns=\"score_type\",\n",
    "            values=\"score_value\",\n",
    "            aggfunc=\"first\"\n",
    "        ).reset_index()\n",
    "\n",
    "    # Apply pivot in a single partition (best if you've already aggregated)\n",
    "    pivot_df = grouped_ddf  # convert to Pandas here\n",
    "    pivot_df = pivot_partition(pivot_df)\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c717384",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df = read_inferred_network(os.path.join(output_dir, \"inferred_grns\", \"inferred_score_df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba26993",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df_string = add_string_db_scores(inferred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79602106",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc90ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe to get it in the same format as combine_dataframes.py\n",
    "score_cols = [\n",
    "    'mean_tf_expr', 'mean_peak_expr', 'mean_tg_expr',\n",
    "    'cicero_score', 'TSS_dist_score', 'correlation',\n",
    "    'LITE_score', 'NITE_score', 'chromatin_differential', \n",
    "    'sliding_window_score', 'homer_binding_score', 'string_experimental_score',\n",
    "    'string_textmining_score', 'string_combined_score'\n",
    "    ]\n",
    "\n",
    "melted_df = inferred_df_string.melt(\n",
    "    id_vars=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "    value_vars=score_cols,\n",
    "    var_name=\"score_type\",\n",
    "    value_name=\"score_value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df.to_parquet(os.path.join(output_dir, \"inferred_grns\", \"inferred_score_df_full.parquet\"), engine=\"pyarrow\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d576e8e",
   "metadata": {},
   "source": [
    "### Investigating if the sliding window scores here are different than the score file\n",
    "\n",
    "**Combined sliding window scores:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b589cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sliding_window_scores = inferred_df_string[[\"source_id\", \"peak_id\", \"sliding_window_score\"]]\n",
    "combined_sliding_window_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff282982",
   "metadata": {},
   "source": [
    "**Scores from the sliding window score file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42be31",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sliding_window_scores_full = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/sliding_window_tf_to_peak_score.parquet\", engine=\"pyarrow\")\n",
    "norm_sliding_window_scores_full[\"source_id\"] = norm_sliding_window_scores_full[\"source_id\"].str.upper()\n",
    "labeled_norm_sliding_window_scores_df = pd.merge(\n",
    "    combined_sliding_window_scores, \n",
    "    norm_sliding_window_scores_full, \n",
    "    on=[\"source_id\", \"peak_id\"], \n",
    "    how=\"inner\"\n",
    "    )\n",
    "labeled_norm_sliding_window_scores_df = labeled_norm_sliding_window_scores_df.rename(columns={\n",
    "    \"sliding_window_score_x\":\"combined_sliding_window_score\",\n",
    "    \"sliding_window_score_y\":\"original_sliding_window_score\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9455aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_norm_sliding_window_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073dbda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_different(row):\n",
    "    return row[\"combined_sliding_window_score\"] != row[\"original_sliding_window_score\"]\n",
    "\n",
    "rows_diff_scores = labeled_norm_sliding_window_scores_df.apply(lambda x: is_different(x), axis=1)\n",
    "matching_scores = len([i for i in rows_diff_scores if i == False])\n",
    "diff_scores = len([i for i in rows_diff_scores if i == True])\n",
    "print(f\"{matching_scores} / {matching_scores + diff_scores} scores are the same between the combined df and the sliding window score df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73238ed",
   "metadata": {},
   "source": [
    "**Labeled score file from the XGBoost code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d68cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_score_df = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/labeled_inferred_grn.parquet\", engine=\"pyarrow\")\n",
    "inferred_edges = combined_score_df[[\"source_id\", \"peak_id\", \"sliding_window_score\", \"label\"]]\n",
    "inferred_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76363a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model_scores = pd.merge(\n",
    "    inferred_edges, \n",
    "    labeled_norm_sliding_window_scores_df,\n",
    "    on=[\"source_id\", \"peak_id\"],\n",
    "    how=\"inner\"\n",
    "    ).rename(columns={\"sliding_window_score\":\"xgboost_sliding_window_score\"})\n",
    "xgboost_model_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_different(row):\n",
    "    return row[\"original_sliding_window_score\"] != row[\"xgboost_sliding_window_score\"]\n",
    "\n",
    "rows_diff_scores = xgboost_model_scores.apply(lambda x: is_different(x), axis=1)\n",
    "matching_scores = len([i for i in rows_diff_scores if i == False])\n",
    "diff_scores = len([i for i in rows_diff_scores if i == True])\n",
    "print(f\"{matching_scores} / {matching_scores + diff_scores} scores are the same between the combined df and the sliding window score df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef6375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_true_false_feature_histogram(\n",
    "    df,\n",
    "    feature_col,\n",
    "    limit_x = True\n",
    "):\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    \n",
    "    true_values = df[df[\"label\"] == 1]\n",
    "    false_values = df[df[\"label\"] == 0]\n",
    "\n",
    "    plt.hist(\n",
    "        false_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.7,\n",
    "        color='#1682b1', edgecolor=\"#032b5f\",\n",
    "        label=\"False\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        true_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.7,\n",
    "        color=\"#cb5f17\", edgecolor=\"#b13301\",\n",
    "        label=\"True\",\n",
    "    )\n",
    "\n",
    "    # set titles/labels on the same ax\n",
    "    plt.title(feature_col, fontsize=14)\n",
    "    plt.xlabel(feature_col, fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    if limit_x:\n",
    "        plt.xlim(0, 1)\n",
    "\n",
    "    fig.legend(\n",
    "        loc=\"lower center\",\n",
    "        ncol=2,\n",
    "        fontsize=14,\n",
    "        bbox_to_anchor=(0.5, -0.02)\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d36c73",
   "metadata": {},
   "source": [
    "Okay, so all of the scores are the same between the XGBoost labeled dataset, the combined score dataset, and the original score file dataset. Perhaps the issue is due to balancing?\n",
    "\n",
    "### Testing if balancing the True / False scores is causing the difference somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba7b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df):\n",
    "    true_rows = df[df[\"label\"] == 1]\n",
    "    false_rows = df[df[\"label\"] == 0]\n",
    "\n",
    "    print(\"Before Balancing:\")\n",
    "    print(f\"  - Number of True values: {len(true_rows)}\")\n",
    "    print(f\"  - Number of False values: {len(false_rows)}\")\n",
    "\n",
    "    min_rows = min(len(true_rows), len(false_rows))\n",
    "    print(f\"\\nSubsampling down to {min_rows} rows\")\n",
    "\n",
    "    true_rows_sampled = true_rows.sample(min_rows)\n",
    "    false_rows_sampled = false_rows.sample(min_rows)\n",
    "\n",
    "    balanced_df = pd.concat([true_rows_sampled, false_rows_sampled])\n",
    "\n",
    "    balanced_true_rows = balanced_df[balanced_df[\"label\"] == 1]\n",
    "    balanced_false_rows = balanced_df[balanced_df[\"label\"] == 0]\n",
    "\n",
    "    print(\"\\nAfter Balancing:\")\n",
    "    print(f\"  - Number of True values: {len(balanced_true_rows)}\")\n",
    "    print(f\"  - Number of False values: {len(balanced_false_rows)}\")\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375119a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model_scores_balanced = balance_dataset(xgboost_model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(xgboost_model_scores_balanced, \"original_sliding_window_score\", limit_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77333b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(xgboost_model_scores_balanced, \"xgboost_sliding_window_score\", limit_x=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bcb4ce",
   "metadata": {},
   "source": [
    "So, now the XGBoost scores are the same as the other ones. I think that it might be due to changing the sliding window score method to only use peak-gene pairs from `peaks_near_genes.parquet`. Let's look into this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_xgboost_scores_labeled = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/previous_labeled_sliding_window_score_df.parquet\", engine=\"pyarrow\")\n",
    "old_xgboost_scores_labeled = old_xgboost_scores_labeled[[\"source_id\", \"peak_id\", \"sliding_window_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc1f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xgboost_scores = xgboost_model_scores_balanced[[\"source_id\", \"peak_id\", \"xgboost_sliding_window_score\", \"label\"]]\n",
    "old_xgboost_model_merged = pd.merge(\n",
    "    new_xgboost_scores, \n",
    "    old_xgboost_scores_labeled,\n",
    "    on=[\"source_id\", \"peak_id\"],\n",
    "    how=\"inner\"\n",
    "    ).rename(columns={\"sliding_window_score\":\"old_xgboost_sliding_window_score\"}).drop_duplicates(subset=[\"source_id\", \"peak_id\"])\n",
    "old_xgboost_model_merged = old_xgboost_model_merged[[\"source_id\", \"peak_id\", \"xgboost_sliding_window_score\", \"old_xgboost_sliding_window_score\", \"label\"]]\n",
    "old_xgboost_model_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47783bbc",
   "metadata": {},
   "source": [
    "Some of the old scores have a sliding window score of 0, which means that there were no motifs found. I'm interested in score differences for edges that had scores in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64143f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_xgboost_model_merged = old_xgboost_model_merged[old_xgboost_model_merged[\"old_xgboost_sliding_window_score\"] > 0]\n",
    "old_xgboost_model_merged = old_xgboost_model_merged[old_xgboost_model_merged[\"xgboost_sliding_window_score\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36139bf2",
   "metadata": {},
   "source": [
    "Let's look at the differences in sliding window scores between the old sliding window scores and the new sliding window scores. I didn't change anything dramatically in the sliding window scoring method, so it should have the same scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_xgboost_model_merged[\"score_difference\"] = old_xgboost_model_merged[\"xgboost_sliding_window_score\"] - old_xgboost_model_merged[\"old_xgboost_sliding_window_score\"]\n",
    "old_xgboost_model_merged[\"score_difference\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a441d9",
   "metadata": {},
   "source": [
    "Negative values mean that the old XGBoost scores were greater, positive values mean that the new XGBoost scores are greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba6545",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_xgboost_model_merged[\"score_difference\"].hist(bins=150, grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19742a",
   "metadata": {},
   "source": [
    "I saved a previous run of the DS011 dataset, so let's compare our scores to those scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46654a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run_scores_df = pd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1_old/sliding_window_tf_to_peak_score.parquet\", engine=\"pyarrow\")\n",
    "old_run_scores_df = old_run_scores_df.reset_index(drop=True)\n",
    "old_run_scores_df[\"source_id\"] = old_run_scores_df[\"source_id\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0259e",
   "metadata": {},
   "source": [
    "We need to merge the edges from the old run to the edges in the new run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46610400",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run_merged = pd.merge(\n",
    "    old_xgboost_model_merged,\n",
    "    old_run_scores_df,\n",
    "    on=[\"source_id\", \"peak_id\"],\n",
    "    how=\"inner\"\n",
    ").rename(columns={\"sliding_window_score\":\"old_run_sliding_window_score\"}).drop_duplicates(subset=[\"source_id\", \"peak_id\"])\n",
    "old_run_merged = old_run_merged[[\"source_id\", \"peak_id\", \"xgboost_sliding_window_score\", \"old_xgboost_sliding_window_score\", \"old_run_sliding_window_score\", \"label\"]]\n",
    "old_run_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4afcc",
   "metadata": {},
   "source": [
    "Now let's see what the differences are between the old run and our most recent scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5fb2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run_merged[\"new_score_vs_old_run_score_difference\"] = old_run_merged[\"xgboost_sliding_window_score\"] - old_run_merged[\"old_run_sliding_window_score\"]\n",
    "old_run_merged[\"new_score_vs_old_run_score_difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "plt.hist(x=old_run_merged[\"new_score_vs_old_run_score_difference\"], bins=25)\n",
    "plt.xlim((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c464b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_run_merged[\"old_xgb_vs_old_run_score_difference\"] = old_run_merged[\"old_xgboost_sliding_window_score\"] - old_run_merged[\"old_run_sliding_window_score\"]\n",
    "old_run_merged[\"old_xgb_vs_old_run_score_difference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f1a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4))\n",
    "plt.hist(x=old_run_merged[\"old_xgb_vs_old_run_score_difference\"], bins=50)\n",
    "plt.xlim((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70489cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_scores = pd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1/sliding_window_tf_to_peak_score.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556166b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f16062",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_scores[\"source_id\"] = full_dataset_scores[\"source_id\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_merged = pd.merge(\n",
    "    inferred_edges,\n",
    "    full_dataset_scores,\n",
    "    on=[\"source_id\", \"peak_id\"],\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae62a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_full_dataset = balance_dataset(full_dataset_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fe7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(balanced_full_dataset, \"sliding_window_score_y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df5543",
   "metadata": {},
   "source": [
    "Let's try using the edges from the sliding window scores that caused the different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_merged_w_diff_dist = pd.merge(\n",
    "    old_xgboost_model_merged,\n",
    "    full_dataset_scores,\n",
    "    on=[\"source_id\", \"peak_id\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "full_dataset_merged_w_diff_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_merged_w_diff_dist = full_dataset_merged_w_diff_dist.rename(columns={\"sliding_window_score\":\"full_dataset_sliding_window_score\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c37346",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_merged_w_diff_dist_balanced = balance_dataset(full_dataset_merged_w_diff_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(full_dataset_merged_w_diff_dist_balanced, \"full_dataset_sliding_window_score\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
