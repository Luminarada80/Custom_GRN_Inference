{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d00c90c",
   "metadata": {},
   "source": [
    "## Plotting Sliding Window Score AUROC and AUPRC\n",
    "\n",
    "When evaluating the feature score distributions of True / False values after adding MIRA RP scores to our combined DataFrame, we noticed that the `sliding_window_score` feature has a good separation in score distribution between True and False values. The True and False values are from the **RN115 LOGOF ESCAPE** mESC knockout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostnamectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "Image(\n",
    "    \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/figures/mm10/DS011/xgboost_feature_score_hist_by_label.png\", \n",
    "    width=800, \n",
    "    height=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21f27f",
   "metadata": {},
   "source": [
    "Now, we are interested in assessing the AUROC and AUPRC for just the sliding window scores.\n",
    "\n",
    "I figured out that the large number of 1 values was due to `clip_and_normalize_log1p_pandas()`. This clips scores below the bottom 5th percentile and above the top 95th percentile and sets them equal to the threshold, then the min-max normalization was moving the distribution to between 0-1. This caused a lot of scores to build up at 0 (the bottom threshold) and 1 (the top threshold).\n",
    "\n",
    "First, we will look at the AUROC and AUPRC as it stands now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99321408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5045420",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df = pd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1/labeled_inferred_grn.parquet\", engine=\"pyarrow\")\n",
    "sliding_window_score_df = inferred_df[[\"source_id\", \"peak_id\", \"target_id\", \"sliding_window_score\", \"label\"]]\n",
    "sliding_window_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606ec77",
   "metadata": {},
   "source": [
    "We first need to balance the number of True and False rows to not skew the accuracy curves as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092548d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df):\n",
    "    true_rows = df[df[\"label\"] == 1]\n",
    "    false_rows = df[df[\"label\"] == 0]\n",
    "\n",
    "    print(\"Before Balancing:\")\n",
    "    print(f\"  - Number of True values: {len(true_rows)}\")\n",
    "    print(f\"  - Number of False values: {len(false_rows)}\")\n",
    "\n",
    "    min_rows = min(len(true_rows), len(false_rows))\n",
    "    print(f\"\\nSubsampling down to {min_rows} rows\")\n",
    "\n",
    "    true_rows_sampled = true_rows.sample(min_rows)\n",
    "    false_rows_sampled = false_rows.sample(min_rows)\n",
    "\n",
    "    balanced_df = pd.concat([true_rows_sampled, false_rows_sampled])\n",
    "\n",
    "    balanced_true_rows = balanced_df[balanced_df[\"label\"] == 1]\n",
    "    balanced_false_rows = balanced_df[balanced_df[\"label\"] == 0]\n",
    "\n",
    "    print(\"\\nAfter Balancing:\")\n",
    "    print(f\"  - Number of True values: {len(balanced_true_rows)}\")\n",
    "    print(f\"  - Number of False values: {len(balanced_false_rows)}\")\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = balance_dataset(sliding_window_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67dbe79",
   "metadata": {},
   "source": [
    "Let's look at the True / False sliding window score histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fda93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_false_feature_histogram(\n",
    "    df,\n",
    "    feature_col,\n",
    "    limit_x = True\n",
    "):\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    \n",
    "    true_values = df[df[\"label\"] == 1]\n",
    "    false_values = df[df[\"label\"] == 0]\n",
    "\n",
    "    plt.hist(\n",
    "        false_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.7,\n",
    "        color='#1682b1', edgecolor=\"#032b5f\",\n",
    "        label=\"False\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        true_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.7,\n",
    "        color=\"#cb5f17\", edgecolor=\"#b13301\",\n",
    "        label=\"True\",\n",
    "    )\n",
    "\n",
    "    # set titles/labels on the same ax\n",
    "    plt.title(feature_col, fontsize=14)\n",
    "    plt.xlabel(feature_col, fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    if limit_x:\n",
    "        plt.xlim(0, 1)\n",
    "\n",
    "    fig.legend(\n",
    "        loc=\"lower center\",\n",
    "        ncol=2,\n",
    "        fontsize=14,\n",
    "        bbox_to_anchor=(0.5, -0.02)\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(balanced_df, \"sliding_window_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64027c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auroc_auprc(inferred_df):\n",
    "    # Subset the relevant columns\n",
    "    df = inferred_df[[\"sliding_window_score\", \"label\"]].dropna()\n",
    "\n",
    "    # Get true labels and predicted scores\n",
    "    y_true = df[\"label\"]\n",
    "    y_scores = df[\"sliding_window_score\"]\n",
    "\n",
    "    # --- ROC Curve ---\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # --- PR Curve ---\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    avg_precision = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # ROC\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f\"AUROC = {roc_auc:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    # PR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label=f\"AUPRC = {avg_precision:.2f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_auroc_auprc(balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57083fc9",
   "metadata": {},
   "source": [
    "As expected, the large number of incorrect 0 and 1 values are impacting the predictions. Let's see what happens if we filter out sliding window scores with values of 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_balanced_df = balanced_df[\n",
    "    (balanced_df[\"sliding_window_score\"] > 0) &\n",
    "    (balanced_df[\"sliding_window_score\"] < 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ee152",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(filtered_balanced_df, \"sliding_window_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auroc_auprc(filtered_balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f37f4",
   "metadata": {},
   "source": [
    "That improves the AUROC and AUPRC by quite a lot. Let's also look at the score distribution with 0 and 1 values removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737cf6f",
   "metadata": {},
   "source": [
    "Now, I need to look into what is causing scores to have a value of 1 or 0. I am re-running the sliding window score method for the dataset without minmax normalizing between 0-1 to get a better look at how the raw scores are distributed without normalization or clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_raw_df = pd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1/no_norm_sliding_window_tf_to_peak_score.parquet\", engine=\"pyarrow\")\n",
    "sliding_window_raw_df = sliding_window_raw_df.reset_index(drop=True)\n",
    "sliding_window_raw_df[\"source_id\"] = sliding_window_raw_df[\"source_id\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4414ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72ebd0",
   "metadata": {},
   "source": [
    "Now that we have the raw sliding window scores, we need to label the edges with the ground truth label. However, the sliding window scores only have peak to TG edges. In order to get TF to TG edges, we can match up the sliding window peak-TG edges to the labeled inferred score dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bf060",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_raw_df[\"source_id\"] = sliding_window_raw_df[\"source_id\"].str.upper()\n",
    "\n",
    "inferred_edges = inferred_df[[\"source_id\", \"peak_id\", \"label\"]]\n",
    "labeled_sliding_window_raw_df = pd.merge(inferred_edges, sliding_window_raw_df, on=[\"source_id\", \"peak_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82711af9",
   "metadata": {},
   "source": [
    "We need to re-balance this dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57851ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = balance_dataset(labeled_sliding_window_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6318ed23",
   "metadata": {},
   "source": [
    "Now we can plot the True / False score histogram and AUROC / AUPRC plots for the balanced labeled raw sliding window scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d669869",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(balanced_df, \"sliding_window_score\", limit_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ec69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df_above_0 = balanced_df[balanced_df[\"sliding_window_score\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(balanced_df_above_0, \"sliding_window_score\", limit_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796f5f9",
   "metadata": {},
   "source": [
    "A lot of the scores were clipped, which led to only the middle of the distributions being kept. The actual distribution is a lot messier than it looks with the log1p normalization and percentile clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auroc_auprc(balanced_df_above_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38838b38",
   "metadata": {},
   "source": [
    "As expected, this reduced the AUROC and AUPRC performance. The AUROC is similar to the one with the clipped values between FPRs of 0.2 and 0.4, as values above and below this were clipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30239b11",
   "metadata": {},
   "source": [
    "## Investigating the True interactions below the bottom 5th percentile\n",
    "\n",
    "We see an enrichment for True scores below a score of 10,000. There are several potential reasons why we might see this pattern, so let's start by investigating the following:\n",
    "\n",
    "1) Are the scores dominated by a few TFs?\n",
    "2) Are the scores dominated by a few TGs?\n",
    "3) Are the scores mainly located on a certain chromosome?\n",
    "4) Are the scores mainly short / long range interactions?\n",
    "\n",
    "First, we need to separate out the True and False scores above and below the bottom 5th percentile and above the top 5th percentile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ddb18",
   "metadata": {},
   "source": [
    "**Loading in the sliding window score file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a40e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sliding_window_scores_full = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/sliding_window_tf_to_peak_score.parquet\", engine=\"pyarrow\")\n",
    "norm_sliding_window_scores_full[\"source_id\"] = norm_sliding_window_scores_full[\"source_id\"].str.upper()\n",
    "norm_sliding_window_scores_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f37bd",
   "metadata": {},
   "source": [
    "**Loading in the combined score DataFrame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dcaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_score_df = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/labeled_inferred_grn.parquet\", engine=\"pyarrow\")\n",
    "combined_score_balanced = balance_dataset(combined_score_df)\n",
    "inferred_edges = balanced_df[[\"source_id\", \"peak_id\", \"sliding_window_score\", \"label\"]]\n",
    "inferred_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8831954",
   "metadata": {},
   "source": [
    "**Merging the score file and the combined score DataFrame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ce064",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_norm_sliding_window_scores_df = pd.merge(inferred_edges, norm_sliding_window_scores_full, on=[\"source_id\", \"peak_id\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_norm_sliding_window_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403dd55",
   "metadata": {},
   "source": [
    "The sliding_window_score distributions look different between the combined inferred GRN score dataframe and the actual sliding window scores, even when I just keep shared edges. Why is that?\n",
    "\n",
    "**Combined score DataFrame sliding window scores:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f7ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(inferred_edges, \"sliding_window_score\", limit_x=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d3a42",
   "metadata": {},
   "source": [
    "**Scores directly form the sliding window score file, edges matched with the combined DataFrame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457dd4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(labeled_norm_sliding_window_scores_df, \"sliding_window_score_y\", limit_x=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546ee66",
   "metadata": {},
   "source": [
    "Let's look at the `inferred_score_df_full.parquet` file created from `explore_score_dataframe_sizes.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f574614",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_score_full_df = pd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1/inferred_grns/inferred_score_df_full.parquet\", engine=\"pyarrow\")\n",
    "inferred_score_full_df = inferred_score_full_df[[\"source_id\", \"peak_id\", \"target_id\", \"sliding_window_score\"]]\n",
    "inferred_score_full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fbf1a4",
   "metadata": {},
   "source": [
    "Let's compare the sliding window score distributions between the combined score dataframe and the raw sliding window scores for shared edges, and see if there are differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873474fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_score_labeled_df = pd.merge(\n",
    "    labeled_norm_sliding_window_scores_df, \n",
    "    inferred_score_full_df, \n",
    "    on=[\"source_id\", \"peak_id\"], \n",
    "    how=\"inner\"\n",
    "    )\n",
    "inferred_score_labeled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864a0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_different(row):\n",
    "    return row[\"sliding_window_score_x\"] != row[\"sliding_window_score_y\"]\n",
    "\n",
    "rows_diff_scores = inferred_score_labeled_df.apply(lambda x: is_different(x), axis=1)\n",
    "matching_scores = len([i for i in rows_diff_scores if i == False])\n",
    "diff_scores = len([i for i in rows_diff_scores if i == True])\n",
    "print(f\"{matching_scores} / {matching_scores + diff_scores} scores are the same between the combined df and the sliding window score df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb21918",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb651c",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_labeled_norm_sliding_window_scores_df = balance_dataset(labeled_norm_sliding_window_scores_df)\n",
    "\n",
    "plot_true_false_feature_histogram(balanced_labeled_norm_sliding_window_scores_df, \"sliding_window_score\", limit_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473abc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_threshold = balanced_df[\"sliding_window_score\"].quantile(0.05)\n",
    "top_threshold = balanced_df[\"sliding_window_score\"].quantile(0.95)\n",
    "\n",
    "scores_below_5th_percentile = balanced_df[balanced_df[\"sliding_window_score\"] <= bottom_threshold]\n",
    "scores_between_thresholds = balanced_df[\n",
    "    (balanced_df[\"sliding_window_score\"] > bottom_threshold) &\n",
    "    (balanced_df[\"sliding_window_score\"] < top_threshold)\n",
    "    ]\n",
    "scores_above_95th_percentile = balanced_df[balanced_df[\"sliding_window_score\"] >= top_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0745d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 4))\n",
    "\n",
    "# Extract the True values within each threshold\n",
    "true_values_below_bottom_thresh = scores_below_5th_percentile[scores_below_5th_percentile[\"label\"] == 1]\n",
    "true_values_between_thresh = scores_between_thresholds[scores_between_thresholds[\"label\"] == 1]\n",
    "true_values_above_top_thresh = scores_above_95th_percentile[scores_above_95th_percentile[\"label\"] == 1]\n",
    "\n",
    "# Extact the False values within each threshold\n",
    "false_values_below_bottom_thresh = scores_below_5th_percentile[scores_below_5th_percentile[\"label\"] == 0]\n",
    "false_values_between_thresh = scores_between_thresholds[scores_between_thresholds[\"label\"] == 0]\n",
    "false_values_above_top_thresh = scores_above_95th_percentile[scores_above_95th_percentile[\"label\"] == 0]\n",
    "\n",
    "# Plot the True / False feature scores BELOW the bottom 5th percentile threshold\n",
    "plt.hist(\n",
    "    false_values_below_bottom_thresh[\"sliding_window_score\"].dropna(),\n",
    "    bins=20, alpha=0.7,\n",
    "    color=\"#759fb1\", edgecolor=\"#1e3b60\",\n",
    ")\n",
    "plt.hist(\n",
    "    true_values_below_bottom_thresh[\"sliding_window_score\"].dropna(),\n",
    "    bins=20, alpha=0.7,\n",
    "    color=\"#ca9f83\", edgecolor=\"#ab5938\",\n",
    ")\n",
    "\n",
    "plt.axvline(x=bottom_threshold, linestyle=\"--\")\n",
    "\n",
    "# Plot the True / False feature scores BETWEEN the 5th and 95th percentiles\n",
    "plt.hist(\n",
    "    false_values_between_thresh[\"sliding_window_score\"].dropna(),\n",
    "    bins=50, alpha=0.7,\n",
    "    color='#1682b1', edgecolor=\"#032b5f\",\n",
    "    label=\"False\",\n",
    ")\n",
    "plt.hist(\n",
    "    true_values_between_thresh[\"sliding_window_score\"].dropna(),\n",
    "    bins=50, alpha=0.7,\n",
    "    color=\"#cb5f17\", edgecolor=\"#b13301\",\n",
    "    label=\"True\",\n",
    ")\n",
    "\n",
    "plt.axvline(x=top_threshold, linestyle=\"--\")\n",
    "\n",
    "# Plot the True / False feature scores ABOVE the top 95th percentile threshold\n",
    "plt.hist(\n",
    "    false_values_above_top_thresh[\"sliding_window_score\"].dropna(),\n",
    "    bins=20, alpha=0.7,\n",
    "    color=\"#759fb1\", edgecolor=\"#1e3b60\",\n",
    ")\n",
    "plt.hist(\n",
    "    true_values_above_top_thresh[\"sliding_window_score\"].dropna(),\n",
    "    bins=20, alpha=0.7,\n",
    "    color=\"#ca9f83\", edgecolor=\"#ab5938\",\n",
    ")\n",
    "\n",
    "\n",
    "# set titles/labels on the same ax\n",
    "plt.title(\"Bottom 5th percentile and upper 95th Percentiles\", fontsize=14)\n",
    "plt.xlabel(\"Sliding Window Score\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "\n",
    "fig.legend(\n",
    "    loc=\"lower center\",\n",
    "    ncol=2,\n",
    "    fontsize=14,\n",
    "    bbox_to_anchor=(0.5, -0.02)\n",
    ")\n",
    "fig.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d330118",
   "metadata": {},
   "source": [
    "As a sanity check, lets make sure that the clipping and normalization creates the same histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a716a43",
   "metadata": {},
   "source": [
    "**Clipping:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_score_df = balanced_df.copy()\n",
    "clipped_score_df[\"sliding_window_score\"] = clipped_score_df[\"sliding_window_score\"].clip(lower=bottom_threshold, upper=top_threshold)\n",
    "plot_true_false_feature_histogram(clipped_score_df, \"sliding_window_score\", limit_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc75088",
   "metadata": {},
   "source": [
    "**Min-Max Normalization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_clipped_score_df = clipped_score_df.copy()\n",
    "norm_clipped_score_df[\"sliding_window_score\"] = (norm_clipped_score_df[\"sliding_window_score\"] - bottom_threshold) / (top_threshold - bottom_threshold)\n",
    "plot_true_false_feature_histogram(norm_clipped_score_df, \"sliding_window_score\", limit_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c02f17",
   "metadata": {},
   "source": [
    "**Log1p Normalization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11288463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log1p_norm_clipped_score_df = norm_clipped_score_df.copy()\n",
    "log1p_norm_clipped_score_df[\"sliding_window_score\"] = np.log1p(log1p_norm_clipped_score_df[\"sliding_window_score\"])\n",
    "plot_true_false_feature_histogram(log1p_norm_clipped_score_df, \"sliding_window_score\", limit_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70416fa5",
   "metadata": {},
   "source": [
    "**Minmax normalization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_log1p_norm_clipped_score_df = log1p_norm_clipped_score_df.copy()\n",
    "min_val = minmax_log1p_norm_clipped_score_df[\"sliding_window_score\"].min()\n",
    "max_val = minmax_log1p_norm_clipped_score_df[\"sliding_window_score\"].max()\n",
    "minmax_log1p_norm_clipped_score_df.loc[:, \"sliding_window_score\"] = (minmax_log1p_norm_clipped_score_df[\"sliding_window_score\"] - min_val) / (max_val - min_val)\n",
    "plot_true_false_feature_histogram(minmax_log1p_norm_clipped_score_df, \"sliding_window_score\", limit_x=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
