{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e69874",
   "metadata": {},
   "source": [
    "# Exploring Score DataFrame Sizes\n",
    "\n",
    "As we combine the output DataFrames from multiple scoring methods, it is useful to know the sizes of each dataframe and how much overlap they have with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7230d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from grn_inference.normalization import minmax_normalize_pandas\n",
    "\n",
    "project_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER\"\n",
    "input_dir = os.path.join(project_dir, \"input/DS011_mESC/DS011_mESC_sample1\")\n",
    "output_dir = os.path.join(project_dir, \"output/DS011_mESC/DS011_mESC_sample1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e64604",
   "metadata": {},
   "source": [
    "## Size of the Input Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa378019",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_processed_df = pd.read_parquet(\n",
    "    os.path.join(input_dir, \"DS011_mESC_ATAC_processed.parquet\"), \n",
    "    engine=\"pyarrow\"\n",
    "    )\n",
    "\n",
    "rna_processed_df = pd.read_parquet(\n",
    "    os.path.join(input_dir, \"DS011_mESC_RNA_processed.parquet\"), \n",
    "    engine=\"pyarrow\"\n",
    "    )\n",
    "\n",
    "print(\"scATAC-seq Dataset:\")\n",
    "print(f\"  - Cells: {atac_processed_df.shape[1]-1}\")\n",
    "print(f\"  - Peaks: {atac_processed_df.shape[0]}\")\n",
    "\n",
    "print(\"\\nscRNA-seq Dataset:\")\n",
    "print(f\"  - Cells: {rna_processed_df.shape[1]-1}\")\n",
    "print(f\"  - Genes: {rna_processed_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afcadbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc442f",
   "metadata": {},
   "source": [
    "## Peak to TG\n",
    "\n",
    "The peak to TG regulatory potential methods we use are:\n",
    "\n",
    "1) Cicero\n",
    "2) TSS Distance\n",
    "3) MIRA\n",
    "4) TG Expression / Peak Accessibility correlation\n",
    "\n",
    "First, let's look at the size of each methods output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_to_tg_score_summary(score_file_name, method_name) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(output_dir, score_file_name)\n",
    "    )\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\")\n",
    "    print(f\"  - Unique TGs: {df['target_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def tf_to_peak_score_summary(score_file_name, method_name) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(output_dir, score_file_name)\n",
    "    )\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique TFs: {df['source_id'].nunique():,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "method_files = {\n",
    "    \"Cicero\": \"cicero_peak_to_tg_scores.parquet\",\n",
    "    \"TSS Distance Score\": \"tss_distance_score.parquet\",\n",
    "    \"MIRA\": \"ds011_peak_to_gene_lite_rp_score_chrom_diff.parquet\",\n",
    "    \"Correlation\": \"peak_to_gene_correlation.parquet\"\n",
    "}\n",
    "\n",
    "method_dfs = {\n",
    "    name: peak_to_tg_score_summary(path, name)\n",
    "    for name, path in method_files.items()\n",
    "}\n",
    "\n",
    "method_peaks = {name: set(df[\"peak_id\"]) for name, df in method_dfs.items()}\n",
    "method_targets = {name: set(df[\"target_id\"]) for name, df in method_dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d80d2c",
   "metadata": {},
   "source": [
    "The TSS distance score DataFrame is much larger than the others. Let's look at how many of the peak to TG edges are shared between scoring methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_elements(method_dict, label):\n",
    "    shared_elements = set()\n",
    "    method_names = list(method_dict.keys())\n",
    "    print(f\"\\nShared {label}s:\")\n",
    "    for i, m1 in enumerate(method_names):\n",
    "        for m2 in method_names[i+1:]:\n",
    "            shared = method_dict[m1] & method_dict[m2]\n",
    "            print(f\"  - {m1} vs {m2}: {len(shared):,}\")\n",
    "            shared_elements.update(shared)\n",
    "    return shared_elements\n",
    "\n",
    "# Compute shared peaks and target genes\n",
    "shared_peaks = get_shared_elements(method_peaks, \"Peak\")\n",
    "shared_targets = get_shared_elements(method_targets, \"Target Gene\")\n",
    "\n",
    "print(f\"\\nPeaks in more than one method: {len(shared_peaks):,}\")\n",
    "print(f\"Target genes in more than one method: {len(shared_targets):,}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64db4a3",
   "metadata": {},
   "source": [
    "There is not a ton of overlap, but this reduces the size of the DataFrame. Now, lets subset each score DataFrame to only use edges that are in more than one scoring method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df_to_peaks_and_targets_in_more_than_one_df(df, method_name, shared_peaks, shared_targets) -> pd.DataFrame:\n",
    "    df = df[\n",
    "    (df[\"peak_id\"].isin(shared_peaks)) &\n",
    "    (df[\"target_id\"].isin(shared_targets))\n",
    "    ].dropna()\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\")\n",
    "    print(f\"  - Unique TGs: {df['target_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Filter each dataframe to only those peak-target pairs that are in multiple methods\n",
    "method_dfs_subset = {\n",
    "    name: subset_df_to_peaks_and_targets_in_more_than_one_df(df, name, shared_peaks, shared_targets)\n",
    "    for name, df in method_dfs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d47707",
   "metadata": {},
   "source": [
    "Once we have only edges found in more than one score DataFrame, we can merge the scoring methods together into a single tf to peak DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_scores_no_tss = pd.merge(\n",
    "    pd.merge(method_dfs_subset[\"Cicero\"], method_dfs_subset[\"MIRA\"], on=[\"peak_id\", \"target_id\"], how=\"outer\"),\n",
    "    method_dfs_subset[\"Correlation\"], on=[\"peak_id\", \"target_id\"], how=\"outer\"\n",
    ")\n",
    "\n",
    "tf_to_peak = pd.merge(\n",
    "    merge_scores_no_tss, \n",
    "    method_dfs_subset[\"TSS Distance Score\"], \n",
    "    on=[\"peak_id\", \"target_id\"], \n",
    "    how=\"inner\"\n",
    "    )\n",
    "tf_to_peak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a414ab",
   "metadata": {},
   "source": [
    "Let's look at how sparse the dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_edges = len(tf_to_peak[[\"peak_id\", \"target_id\"]])\n",
    "score_cols = {\n",
    "    \"Cicero Score\": \"cicero_score\",\n",
    "    \"TSS Distance Score\": \"TSS_dist_score\",\n",
    "    \"MIRA LITE Score\": \"MIRA_LITE_RP_score\",\n",
    "    \"MIRA Chromatin Differential Score\": \"chromatin_differential\",\n",
    "    \"Correlation Score\": \"correlation\"\n",
    "}\n",
    "\n",
    "print(f\"Total Edges: {total_edges:,}\")\n",
    "for label, col in score_cols.items():\n",
    "    score_count = tf_to_peak[col].notna().sum()\n",
    "    print(f\"  - {label}: {score_count:,} ({score_count / total_edges:.2%} of total scores)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373f404",
   "metadata": {},
   "source": [
    "The merged dataset isn't too sparse if we outer merge the method score DataFrames, then use an inner merge for the TSS distance scores. The number of peak to TG edges has dropped dramatically, which may or may not be a good thing. We can add a column called `\"support_count\"` which counts the number methods with non-nan values across rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b374869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many scores are non-null for each edge\n",
    "score_support_counts = tf_to_peak[score_cols.values()].notna().sum(axis=1)\n",
    "\n",
    "# Select edges supported by 2 or more methods\n",
    "for i, _ in enumerate(score_cols.values()):\n",
    "    if i > 0:\n",
    "        multi_supported_edges = tf_to_peak[score_support_counts >= i]\n",
    "\n",
    "        print(f\"Edges supported by â‰¥{i} methods: {len(multi_supported_edges):,} out of {len(tf_to_peak):,}\")\n",
    "tf_to_peak[\"support_count\"] = score_support_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750eb66",
   "metadata": {},
   "source": [
    "Now that we have a DataFrame containing all of the peak to TG scores for at least 2 methods, we can add in the **minmax normalized mean peak accessibility and mean TG expression** for each edge. We use an inner join here to only keep rows that have genes and peaks in the expression and accesibility data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc718024",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_peak_expr_norm = minmax_normalize_pandas(\n",
    "    atac_processed_df.set_index(\"peak_id\").mean(axis=1).rename(\"mean_peak_expr\").to_frame(),\n",
    "    [\"mean_peak_expr\"]\n",
    ").reset_index()\n",
    "\n",
    "mean_gene_expr_norm = minmax_normalize_pandas(\n",
    "    rna_processed_df.set_index(\"gene_id\").mean(axis=1).rename(\"mean_gene_expr\").to_frame(),\n",
    "    [\"mean_gene_expr\"]\n",
    ").reset_index()\n",
    "\n",
    "tf_to_peak_mean_acc = pd.merge(tf_to_peak, mean_peak_expr_norm, on=\"peak_id\", how=\"inner\")\n",
    "full_tf_to_peak = pd.merge(tf_to_peak_mean_acc, mean_gene_expr_norm, left_on=\"target_id\", right_on=\"gene_id\", how=\"inner\").drop(columns=\"gene_id\")\n",
    "full_tf_to_peak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74063ed8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3512676",
   "metadata": {},
   "source": [
    "## TF to Peak\n",
    "\n",
    "Now that we have the peak to TG scores together, we need to combine the TF to peak binding potential scores. We have two methods that calculate TF to peak scores:\n",
    "\n",
    "1) Homer\n",
    "2) Sliding Window\n",
    "\n",
    "Let's start by loading these score DataFrames in and take a look at their sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
