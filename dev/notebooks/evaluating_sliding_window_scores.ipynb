{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d00c90c",
   "metadata": {},
   "source": [
    "## Plotting Sliding Window Score AUROC and AUPRC\n",
    "\n",
    "When evaluating the feature score distributions of True / False values after adding MIRA RP scores to our combined DataFrame, we noticed that the `sliding_window_score` feature has a good separation in score distribution between True and False values. The True and False values are from the **RN115 LOGOF ESCAPE** mESC knockout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47f3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostnamectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "Image(\n",
    "    \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/figures/mm10/DS011/xgboost_feature_score_hist_by_label.png\", \n",
    "    width=800, \n",
    "    height=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21f27f",
   "metadata": {},
   "source": [
    "Now, we are interested in assessing the AUROC and AUPRC for just the sliding window scores.\n",
    "\n",
    "I figured out that the large number of 1 values was due to `clip_and_normalize_log1p_pandas()`. This clips scores below the bottom 5th percentile and above the top 95th percentile and sets them equal to the threshold, then the min-max normalization was moving the distribution to between 0-1. This caused a lot of scores to build up at 0 (the bottom threshold) and 1 (the top threshold).\n",
    "\n",
    "First, we will look at the AUROC and AUPRC as it stands now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99321408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5045420",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df = pd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1/labeled_inferred_grn.parquet\", engine=\"pyarrow\")\n",
    "sliding_window_score_df = inferred_df[[\"source_id\", \"peak_id\", \"target_id\", \"sliding_window_score\", \"label\"]]\n",
    "sliding_window_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606ec77",
   "metadata": {},
   "source": [
    "We first need to balance the number of True and False rows to not skew the accuracy curves as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092548d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(df):\n",
    "    true_rows = df[df[\"label\"] == 1]\n",
    "    false_rows = df[df[\"label\"] == 0]\n",
    "\n",
    "    print(\"Before Balancing:\")\n",
    "    print(f\"  - Number of True values: {len(true_rows)}\")\n",
    "    print(f\"  - Number of False values: {len(false_rows)}\")\n",
    "\n",
    "    min_rows = min(len(true_rows), len(false_rows))\n",
    "    print(f\"\\nSubsampling down to {min_rows} rows\")\n",
    "\n",
    "    true_rows_sampled = true_rows.sample(min_rows)\n",
    "    false_rows_sampled = false_rows.sample(min_rows)\n",
    "\n",
    "    balanced_df = pd.concat([true_rows_sampled, false_rows_sampled])\n",
    "\n",
    "    balanced_true_rows = balanced_df[balanced_df[\"label\"] == 1]\n",
    "    balanced_false_rows = balanced_df[balanced_df[\"label\"] == 0]\n",
    "\n",
    "    print(\"\\nAfter Balancing:\")\n",
    "    print(f\"  - Number of True values: {len(balanced_true_rows)}\")\n",
    "    print(f\"  - Number of False values: {len(balanced_false_rows)}\")\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = balance_dataset(sliding_window_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67dbe79",
   "metadata": {},
   "source": [
    "Let's look at the True / False sliding window score histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fda93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_false_feature_histogram(\n",
    "    df,\n",
    "    feature_col,\n",
    "    limit_x = True\n",
    "):\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    \n",
    "    true_values = df[df[\"label\"] == 1]\n",
    "    false_values = df[df[\"label\"] == 0]\n",
    "\n",
    "    plt.hist(\n",
    "        false_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.7,\n",
    "        color='#1682b1', edgecolor=\"#032b5f\",\n",
    "        label=\"False\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        true_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.7,\n",
    "        color=\"#cb5f17\", edgecolor=\"#b13301\",\n",
    "        label=\"True\",\n",
    "    )\n",
    "\n",
    "    # set titles/labels on the same ax\n",
    "    plt.title(feature_col, fontsize=14)\n",
    "    plt.xlabel(feature_col, fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    if limit_x:\n",
    "        plt.xlim(0, 1)\n",
    "\n",
    "    fig.legend(\n",
    "        loc=\"lower center\",\n",
    "        ncol=2,\n",
    "        fontsize=14,\n",
    "        bbox_to_anchor=(0.5, -0.02)\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(balanced_df, \"sliding_window_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64027c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auroc_auprc(inferred_df):\n",
    "    # Subset the relevant columns\n",
    "    df = inferred_df[[\"sliding_window_score\", \"label\"]].dropna()\n",
    "\n",
    "    # Get true labels and predicted scores\n",
    "    y_true = df[\"label\"]\n",
    "    y_scores = df[\"sliding_window_score\"]\n",
    "\n",
    "    # --- ROC Curve ---\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # --- PR Curve ---\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    avg_precision = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # ROC\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f\"AUROC = {roc_auc:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    # PR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label=f\"AUPRC = {avg_precision:.2f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_auroc_auprc(balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57083fc9",
   "metadata": {},
   "source": [
    "As expected, there are a large number of incorrect 0 and 1 values from scores above the 95th percentile clipping threshold and below the 5th percentile clipping threshold. Let's see what happens if we filter out sliding window scores with values of 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_balanced_df = balanced_df[\n",
    "    (balanced_df[\"sliding_window_score\"] > 0) &\n",
    "    (balanced_df[\"sliding_window_score\"] < 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ee152",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(filtered_balanced_df, \"sliding_window_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auroc_auprc(filtered_balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1002fd4a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f37f4",
   "metadata": {},
   "source": [
    "### Investigating the bimodal distribution of True scores\n",
    "\n",
    "In the sliding window score distribution, the False scores have a single broad peak around 0.6, while the True scores are bimodal with a lower peak around 0.375 - 0.425 and a higher peak around 0.8 - 0.9. We want to determine what is causing the True values to have multiple peaks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbc6b5",
   "metadata": {},
   "source": [
    "First, let's look at the number of TFs in the True and False scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_true_tfs = sliding_window_score_df[sliding_window_score_df[\"label\"] == 1][\"source_id\"].unique()\n",
    "total_false_tfs = sliding_window_score_df[sliding_window_score_df[\"label\"] == 0][\"source_id\"].unique()\n",
    "\n",
    "print(f\"Total TFs in the True values: {len(total_true_tfs)}\")\n",
    "print(f\"Total TFs in the False values: {len(total_false_tfs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4b2fc5",
   "metadata": {},
   "source": [
    "There are many more TFs in the False values. How many edges does each TF have for the True and False groups?\n",
    "\n",
    "To start answering this question, we can count the number of sliding window scores per TF in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f374b7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_sorted_agg_df = (\n",
    "    sliding_window_score_df[sliding_window_score_df[\"label\"] == 1][[\"source_id\", \"sliding_window_score\"]]\n",
    "    .groupby(\"source_id\")\n",
    "    .count()\n",
    "    .sort_values(by=\"sliding_window_score\", ascending=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sliding_window_score\":\"num_scores\"})\n",
    "    )\n",
    "\n",
    "false_sorted_agg_df = (\n",
    "    sliding_window_score_df[sliding_window_score_df[\"label\"] == 0][[\"source_id\", \"sliding_window_score\"]]\n",
    "    .groupby(\"source_id\")\n",
    "    .count()\n",
    "    .sort_values(by=\"sliding_window_score\", ascending=False)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"sliding_window_score\":\"num_scores\"})\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a538a",
   "metadata": {},
   "source": [
    "Now, we can plot the number of scores per TF\n",
    "\n",
    "#### Number of False sliding window scores by TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.bar(x=false_sorted_agg_df[\"source_id\"], height=false_sorted_agg_df[\"num_scores\"], color=\"blue\")\n",
    "plt.title(\"Number of False sliding window scores by TF\")\n",
    "plt.ylabel(\"Number of False \\nsliding window scores\", fontsize=12)\n",
    "plt.xticks(rotation=55, fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292ae3b",
   "metadata": {},
   "source": [
    "#### Number of True sliding window scores by TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,3))\n",
    "plt.bar(x=true_sorted_agg_df[\"source_id\"], height=true_sorted_agg_df[\"num_scores\"], color=\"blue\")\n",
    "plt.title(\"Number of True sliding window scores by TF\")\n",
    "plt.ylabel(\"Number of True \\nsliding window scores\")\n",
    "plt.xticks(rotation=55, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f63626",
   "metadata": {},
   "source": [
    "It looks like there is a relatively small proportion of True values out of the total number of False values. Let's look at the number of scores for TFs with both True and False values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32633a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_values = pd.merge(true_sorted_agg_df, false_sorted_agg_df, on=\"source_id\", how=\"inner\")\n",
    "grouped_values = grouped_values.rename(columns={\n",
    "    \"num_scores_x\": \"Num True Scores\",\n",
    "    \"num_scores_y\": \"Num False Scores\"\n",
    "})\n",
    "grouped_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e550f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_values.plot.bar(x=\"source_id\", stacked=False, figsize=(7,4))\n",
    "plt.title(\"Number of True and False sliding window scores for TFs with True values\")\n",
    "plt.ylabel(\"Number of \\nsliding window scores\", fontsize=12)\n",
    "plt.xticks(rotation=55, fontsize=10)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0706b4c",
   "metadata": {},
   "source": [
    "There are many more scores for the False values. Let's look to see if there are any differences in the True vs False score distributions for each TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e070ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def plot_tf_score_distributions(df, tf_name_list, score_col, title):\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(len(tf_name_list) / ncols)\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    fig.set_figwidth(ncols * 3)\n",
    "    fig.set_figheight(nrows * 3)\n",
    "\n",
    "    for i, tf_name in enumerate(tf_name_list):\n",
    "        plot_row = i // ncols\n",
    "        plot_col = i % ncols\n",
    "        \n",
    "        tf_scores = df[df[\"source_id\"] == tf_name][score_col]\n",
    "        \n",
    "        ax[plot_row, plot_col].hist(tf_scores, bins=25)\n",
    "        ax[plot_row, plot_col].set_title(tf_name, fontsize=10)\n",
    "        ax[plot_row, plot_col].tick_params(axis='x', labelsize=9)\n",
    "        ax[plot_row, plot_col].tick_params(axis='y', labelsize=9)\n",
    "        ax[plot_row, plot_col].set_xbound((0, 1))\n",
    "\n",
    "    # Hide the extra plots\n",
    "    n_tfs = len(tf_name_list)\n",
    "    n_figs = ncols * nrows\n",
    "\n",
    "    for i in range(n_tfs, n_figs):\n",
    "        row = i // ncols\n",
    "        col = i % ncols\n",
    "        ax[row, col].axis(\"off\")\n",
    "        \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout(rect=[0.05, 0.05, 1, 1])\n",
    "    \n",
    "    fig.text(0.5, 0.04, 'Sliding Window Score', ha='center', fontsize=12)\n",
    "    fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=12)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9f13d",
   "metadata": {},
   "source": [
    "#### Distribution of True sliding window scores per TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = sliding_window_score_df[sliding_window_score_df[\"label\"] == 1]\n",
    "tfs_with_true_scores = true_df[\"source_id\"].unique()\n",
    "plot_tf_score_distributions(\n",
    "    df=true_df, \n",
    "    tf_name_list=tfs_with_true_scores, \n",
    "    score_col=\"sliding_window_score\",\n",
    "    title=\"True sliding window score distributions by TF\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e1a92",
   "metadata": {},
   "source": [
    "#### Distribution of False sliding window scores per TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a765996",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_df = sliding_window_score_df[sliding_window_score_df[\"label\"] == 0]\n",
    "plot_tf_score_distributions(\n",
    "    df=false_df, \n",
    "    tf_name_list=tfs_with_true_scores, \n",
    "    score_col=\"sliding_window_score\",\n",
    "    title=\"False sliding window score distributions by TF\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7274111f",
   "metadata": {},
   "source": [
    "The True and False scores appear to have similar distributions for each of the TFs.\n",
    "\n",
    "For the True scores, only the TFs **SOX2**, **SOX9**, and **TCF3** have more than 10 scores. It appears as though these scores are the drivers of the three peaks that we see on the plot. Let's see how the distributions of these scores match up when plotted together. \n",
    "\n",
    ">Note: The number of True and False scores per TF are balanced for the following plot, otherwise the False edges overwhelm the True edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a19d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def plot_true_false_tf_score_distributions(true_df, false_df, tf_name_list, score_col, title):\n",
    "    ncols = min(len(tf_name_list), 4)\n",
    "    nrows = math.ceil(len(tf_name_list) / ncols)\n",
    "    fig, ax = plt.subplots(nrows=nrows, ncols=ncols)\n",
    "    fig.set_figwidth(ncols * 3)\n",
    "    fig.set_figheight(nrows * 3)\n",
    "    \n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for i, tf_name in enumerate(tf_name_list):\n",
    "        plot_row = i // ncols\n",
    "        plot_col = i % ncols\n",
    "        \n",
    "        true_tf_scores = true_df[true_df[\"source_id\"] == tf_name][score_col]\n",
    "        false_tf_scores = false_df[false_df[\"source_id\"] == tf_name][score_col]\n",
    "        \n",
    "        min_scores = min(len(true_tf_scores), len(false_tf_scores))\n",
    "        true_tf_scores = true_tf_scores.sample(min_scores)\n",
    "        false_tf_scores = false_tf_scores.sample(min_scores)\n",
    "        \n",
    "        ax[i].hist(true_tf_scores, bins=25, alpha=0.7, label=\"True Scores\")\n",
    "        ax[i].hist(false_tf_scores, bins=25, alpha=0.7, label=\"False Scores\")\n",
    "        \n",
    "        ax[i].set_title(tf_name, fontsize=10)\n",
    "        ax[i].tick_params(axis='x', labelsize=9)\n",
    "        ax[i].tick_params(axis='y', labelsize=9)\n",
    "        ax[i].set_xbound((0, 1))\n",
    "\n",
    "    for j in range(len(tf_name_list), len(ax)):\n",
    "        ax[j].axis(\"off\")\n",
    "    \n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    fig.legend(\n",
    "        handles,\n",
    "        labels,\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, 1.0),\n",
    "        ncol=2,\n",
    "        fontsize=10,\n",
    "        frameon=False\n",
    "    )\n",
    "        \n",
    "    plt.suptitle(title, y=1.05)\n",
    "    plt.tight_layout(rect=[0.05, 0.05, 1, 0.98])\n",
    "        \n",
    "    fig.text(0.5, 0.04, 'Sliding Window Score', ha='center', fontsize=11)\n",
    "    fig.text(0.04, 0.5, 'Frequency', va='center', rotation='vertical', fontsize=11)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61930941",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_names = [\"SOX2\", \"SOX9\", \"TCF3\"]\n",
    "\n",
    "plot_true_false_tf_score_distributions(\n",
    "    true_df=true_df,\n",
    "    false_df=false_df,\n",
    "    tf_name_list=tf_names,\n",
    "    score_col=\"sliding_window_score\",\n",
    "    title=\"Sliding window score distributions for TFs with True values\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d765725",
   "metadata": {},
   "source": [
    "Let's plot the balanced distributions again, but this time we can color SOX2, SOX9, and TCF3 differently so we can see if they truly are the majority of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d3aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfs_of_interest(\n",
    "    df,\n",
    "    feature_col,\n",
    "    tfs_of_interest,\n",
    "    limit_x = True\n",
    "):\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    \n",
    "    true_values = df[df[\"label\"] == 1]\n",
    "    false_values = df[df[\"label\"] == 0]\n",
    "\n",
    "    plt.hist(\n",
    "        false_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.3,\n",
    "        color=\"#757575\",\n",
    "        label=\"False Scores\"\n",
    "    )\n",
    "    \n",
    "    y_cmap = plt.get_cmap(\"Dark2\")\n",
    "    \n",
    "    for x, tf in enumerate(tfs_of_interest):\n",
    "        \n",
    "        \n",
    "        true_tfs = true_values[true_values[\"source_id\"] == tf]\n",
    "        \n",
    "        percent_total_true_edges = len(true_tfs[feature_col]) / len(true_values[feature_col])\n",
    "        \n",
    "        nbins=max(10, math.ceil(50*percent_total_true_edges))\n",
    "        \n",
    "        plt.hist(\n",
    "            true_tfs[feature_col].dropna(),\n",
    "            bins=nbins, alpha=0.8,\n",
    "            color=y_cmap.colors[x],\n",
    "            label=tf\n",
    "        )\n",
    "\n",
    "    # set titles/labels on the same ax\n",
    "    plt.title(\"Sliding window score distribution colored by TFs of interest\", fontsize=12)\n",
    "    plt.xlabel(\"Sliding Window Score\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    if limit_x:\n",
    "        plt.xlim(0, 1)\n",
    "\n",
    "    fig.legend(\n",
    "        loc=\"lower center\",\n",
    "        ncol=1,\n",
    "        fontsize=10,\n",
    "        bbox_to_anchor=(1.10, 0.60)\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0, 1, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfs_of_interest(balanced_df, \"sliding_window_score\", tf_names, limit_x=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a90a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(balanced_df, \"sliding_window_score\", limit_x=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f155f00a",
   "metadata": {},
   "source": [
    "Looking at the plot of only the values of the main True TFs, we can see that the three main distributions of True values stems from those TFs having distinct distributions and representing the majority of True edges.\n",
    "\n",
    "Also, if we don't balance the True and False edges, we can see that the True values represent a small majority of the total number of scores. This shows that balancing the scores makes it appear that the True and False scores are more separated than they truly are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(sliding_window_score_df, \"sliding_window_score\", limit_x=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
