{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d00c90c",
   "metadata": {},
   "source": [
    "## Plotting Sliding Window Score AUROC and AUPRC\n",
    "\n",
    "When evaluating the feature score distributions of True / False values after adding MIRA RP scores to our combined DataFrame, we noticed that the `sliding_window_score` feature has a good separation in score distribution between True and False values. The True and False values are from the **RN115 LOGOF ESCAPE** mESC knockout dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d1cf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "Image(\n",
    "    \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/figures/mm10/DS011/xgboost_feature_score_hist_by_label.png\", \n",
    "    width=800, \n",
    "    height=800\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21f27f",
   "metadata": {},
   "source": [
    "Now, we are interested in assessing the AUROC and AUPRC for just the sliding window scores. We also need to asses why a large number of False values have score of 1 and a large number of True values have a score of 0.\n",
    "\n",
    "First, we will look at the AUROC and AUPRC as it stands now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99321408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5045420",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df = pd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1/labeled_inferred_grn.parquet\", engine=\"pyarrow\")\n",
    "sliding_window_score_df = inferred_df[[\"source_id\", \"peak_id\", \"target_id\", \"sliding_window_score\", \"label\"]]\n",
    "sliding_window_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606ec77",
   "metadata": {},
   "source": [
    "We first need to balance the number of True and False rows to not skew the accuracy curves as much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rows = inferred_df[inferred_df[\"label\"] == 1]\n",
    "false_rows = inferred_df[inferred_df[\"label\"] == 0]\n",
    "\n",
    "print(\"Before Balancing:\")\n",
    "print(f\"  - Number of True values: {len(true_rows)}\")\n",
    "print(f\"  - Number of False values: {len(false_rows)}\")\n",
    "\n",
    "min_rows = min(len(true_rows), len(false_rows))\n",
    "print(f\"\\nSubsampling down to {min_rows} rows\")\n",
    "\n",
    "true_rows_sampled = true_rows.sample(min_rows)\n",
    "false_rows_sampled = false_rows.sample(min_rows)\n",
    "\n",
    "balanced_df = pd.concat([true_rows_sampled, false_rows_sampled])\n",
    "\n",
    "balanced_true_rows = balanced_df[balanced_df[\"label\"] == 1]\n",
    "balanced_false_rows = balanced_df[balanced_df[\"label\"] == 0]\n",
    "\n",
    "print(\"\\nAfter Balancing:\")\n",
    "print(f\"  - Number of True values: {len(balanced_true_rows)}\")\n",
    "print(f\"  - Number of False values: {len(balanced_false_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67dbe79",
   "metadata": {},
   "source": [
    "Let's look at the True / False sliding window score histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fda93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_true_false_feature_histogram(\n",
    "    df,\n",
    "    feature_col,\n",
    "):\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 4))\n",
    "    \n",
    "    true_values = df[df[\"label\"] == 1]\n",
    "    false_values = df[df[\"label\"] == 0]\n",
    "\n",
    "    plt.hist(\n",
    "        true_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.7,\n",
    "        color='#1682b1', edgecolor=\"#032b5f\",\n",
    "        label=\"False\",\n",
    "    )\n",
    "    plt.hist(\n",
    "        false_values[feature_col].dropna(),\n",
    "        bins=50, alpha=0.7,\n",
    "        color=\"#cb5f17\", edgecolor=\"#b13301\",\n",
    "        label=\"True\",\n",
    "    )\n",
    "\n",
    "    # set titles/labels on the same ax\n",
    "    plt.title(feature_col, fontsize=14)\n",
    "    plt.xlabel(feature_col, fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    plt.xlim(0, 1)\n",
    "\n",
    "    fig.legend(\n",
    "        loc=\"lower center\",\n",
    "        ncol=2,\n",
    "        fontsize=14,\n",
    "        bbox_to_anchor=(0.5, -0.02)\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "    plt.show()\n",
    "\n",
    "plot_true_false_feature_histogram(balanced_df, \"sliding_window_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64027c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auroc_auprc(inferred_df):\n",
    "    # Subset the relevant columns\n",
    "    df = inferred_df[[\"sliding_window_score\", \"label\"]].dropna()\n",
    "\n",
    "    # Get true labels and predicted scores\n",
    "    y_true = df[\"label\"]\n",
    "    y_scores = df[\"sliding_window_score\"]\n",
    "\n",
    "    # --- ROC Curve ---\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # --- PR Curve ---\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    avg_precision = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    # ROC\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label=f\"AUROC = {roc_auc:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    # PR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label=f\"AUPRC = {avg_precision:.2f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_auroc_auprc(balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57083fc9",
   "metadata": {},
   "source": [
    "As expected, the large number of incorrect 0 and 1 values are impacting the predictions. Let's see what happens if we filter out sliding window scores with values of 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_balanced_df = balanced_df[\n",
    "    (balanced_df[\"sliding_window_score\"] > 0) &\n",
    "    (balanced_df[\"sliding_window_score\"] < 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ee152",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(filtered_balanced_df, \"sliding_window_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163cfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_auroc_auprc(filtered_balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6f37f4",
   "metadata": {},
   "source": [
    "That improves the AUROC and AUPRC by quite a lot. Let's also look at the score distribution with 0 and 1 values removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdabde7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_true_false_feature_histogram(filtered_balanced_df, \"sliding_window_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737cf6f",
   "metadata": {},
   "source": [
    "Now, I need to look into what is causing scores to have a value of 1 or 0. I am re-running the sliding window score method for the dataset without minmax normalizing between 0-1 to get a better look at what the raw scores are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_raw_df = pd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1/sliding_window_tf_to_peak_score.parquet\", engine=\"pyarrow\")\n",
    "sliding_window_raw_df = sliding_window_raw_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4414ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_raw_df = sliding_window_raw_df.reset_index(drop=True)\n",
    "sliding_window_raw_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
