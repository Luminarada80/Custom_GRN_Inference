{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ee069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "COMBINED_SCORE_DF_FEATURES = [\n",
    "    \"mean_TF_expression\", \n",
    "    \"mean_peak_accessibility\", \n",
    "    \"mean_TG_expression\",\n",
    "    \"string_combined_score\",\n",
    "    \"string_experimental_score\",\n",
    "    \"string_textmining_score\"\n",
    "    ]\n",
    "\n",
    "main_input_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/input\"\n",
    "main_output_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output\"\n",
    "\n",
    "ds011_input_dir = os.path.join(main_input_dir, \"DS011_mESC/DS011_mESC_sample1\")\n",
    "ds011_output_dir = os.path.join(main_output_dir, \"DS011_mESC/DS011_mESC_sample1\")\n",
    "\n",
    "mesc_input_dir = os.path.join(main_input_dir, \"mESC/filtered_L2_E7.5_rep2\")\n",
    "mesc_output_dir = os.path.join(main_output_dir, \"mESC/filtered_L2_E7.5_rep2\")\n",
    "\n",
    "def create_score_path_dict(\n",
    "    selected_features: list[str], \n",
    "    output_dir: str\n",
    "    ) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Creates a dictionary of file paths to each score file for a given set of selected features.\n",
    "\n",
    "    Arguments:\n",
    "        selected_features (list[str]): List of selected feature score names\n",
    "        output_dir (str): Output directory for the sample\n",
    "\n",
    "    Returns:\n",
    "        selected_feature_path_dict (dict[str:str]): A dictionary containing the selected feature names\n",
    "        along with the path to the data file for that feature\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_score_file_path_dict = {\n",
    "        'mean_TF_expression' : os.path.join(output_dir, \"inferred_grns/inferred_score_df.parquet\"),\n",
    "        'mean_peak_accessibility' : os.path.join(output_dir, \"inferred_grns/inferred_score_df.parquet\"),\n",
    "        'mean_TG_expression' : os.path.join(output_dir, \"inferred_grns/inferred_score_df.parquet\"),\n",
    "        'cicero_score' : os.path.join(output_dir, \"cicero_peak_to_tg_scores.parquet\"),\n",
    "        'TSS_dist_score' : os.path.join(output_dir, \"peak_to_gene_correlation.parquet\"), \n",
    "        'correlation' : os.path.join(output_dir, \"peak_to_gene_correlation.parquet\"),\n",
    "        'homer_binding_score' : os.path.join(output_dir, \"homer_tf_to_peak.parquet\"), \n",
    "        'sliding_window_score' : os.path.join(output_dir, \"sliding_window_tf_to_peak_score.parquet\"), \n",
    "        'string_combined_score' : os.path.join(output_dir, \"inferred_grns/inferred_score_df.parquet\"), \n",
    "        'string_experimental_score' : os.path.join(output_dir, \"inferred_grns/inferred_score_df.parquet\"), \n",
    "        'string_textmining_score' : os.path.join(output_dir, \"inferred_grns/inferred_score_df.parquet\")\n",
    "    }\n",
    "\n",
    "    selected_feature_path_dict = {}\n",
    "    for feature_name in selected_features:\n",
    "        if feature_name in feature_score_file_path_dict.keys():\n",
    "            selected_feature_path_dict[feature_name] = feature_score_file_path_dict[feature_name]\n",
    "            \n",
    "    for feature_name, path in selected_feature_path_dict.items():\n",
    "        assert os.path.isfile(path) | os.path.isdir(path), f'Error: {path} is not a file or directory'\n",
    "        \n",
    "    return selected_feature_path_dict\n",
    "\n",
    "def check_for_features_in_combined_score_df(feature_score_dict: dict[str,str]):\n",
    "    features = []\n",
    "    for score_name in feature_score_dict.keys():\n",
    "        if score_name in COMBINED_SCORE_DF_FEATURES:\n",
    "            print(f'  - {score_name} in feature score path')\n",
    "            features.append(score_name)\n",
    "            \n",
    "    return features\n",
    "\n",
    "def load_melted_inferred_grn_ddf(inferred_net_path: str, feature_scores: list[str]):\n",
    "\n",
    "    melted_tf_score_ddf = dd.read_parquet(inferred_net_path, engine=\"pyarrow\")\n",
    "    \n",
    "    melted_tf_score_ddf = melted_tf_score_ddf[melted_tf_score_ddf[\"score_type\"].isin(feature_scores)]\n",
    "    \n",
    "    grouped = (\n",
    "        melted_tf_score_ddf\n",
    "        .groupby([\"source_id\", \"peak_id\", \"target_id\", \"score_type\"])\n",
    "        [\"score_value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    pdf = grouped.compute()\n",
    "    \n",
    "    wide_df = pdf.pivot_table(\n",
    "        index=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "        columns=\"score_type\",\n",
    "        values=\"score_value\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "    \n",
    "    \n",
    "    return wide_df\n",
    "\n",
    "def load_individual_score_dataframes(score_path_dict):\n",
    "    individual_feature_score_dataframes = {}\n",
    "    for feature_name, path in score_path_dict.items():\n",
    "        if feature_name not in COMBINED_SCORE_DF_FEATURES:\n",
    "            print(f'  - Loading {feature_name} DataFrame')\n",
    "            df = pd.read_parquet(path, engine=\"pyarrow\")\n",
    "            df = df.reset_index(drop=True)\n",
    "            individual_feature_score_dataframes[feature_name] = df\n",
    "\n",
    "    return individual_feature_score_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d268a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'mean_TF_expression',\n",
    "    'mean_peak_accessibility',\n",
    "    'mean_TG_expression',\n",
    "    'cicero_score',\n",
    "    'TSS_dist_score', \n",
    "    'correlation',\n",
    "    'homer_binding_score', \n",
    "    'sliding_window_score', \n",
    "    'string_combined_score', \n",
    "    'string_experimental_score', \n",
    "    'string_textmining_score'\n",
    "    ]\n",
    "\n",
    "mesc_score_paths: dict = create_score_path_dict(selected_features, mesc_output_dir)\n",
    "\n",
    "print(\"\\nLoading and combining individual feature scores\")\n",
    "feature_score_dataframes: dict = load_individual_score_dataframes(mesc_score_paths)\n",
    "\n",
    "print('\\nChecking for scores originating from the combined feature score DataFrame')\n",
    "combined_df_features: list[str] = check_for_features_in_combined_score_df(mesc_score_paths)\n",
    "\n",
    "if len(combined_df_features) > 0:\n",
    "    print('\\nLoading combined feature score dataframe')\n",
    "    inferred_df_path = mesc_score_paths[combined_df_features[0]] # get path for the first feature name in combined_df_feature\n",
    "    inferred_df = load_melted_inferred_grn_ddf(inferred_df_path, combined_df_features)\n",
    "    print('\\tDone!')\n",
    "    \n",
    "    print(\"\\nSplitting off combined scores into individual dataframes\")\n",
    "    for feature_name in combined_df_features:\n",
    "        feature_df = inferred_df[[\"source_id\", \"peak_id\", \"target_id\", feature_name]]\n",
    "        feature_score_dataframes[feature_name] = feature_df\n",
    "    \n",
    "else:\n",
    "    print('  - No scores from the combined feature score DataFrame')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "cicero_peak_to_tg = feature_score_dataframes[\"cicero_score\"][[\"peak_id\", \"target_id\"]]\n",
    "corr_peak_to_tg = feature_score_dataframes[\"correlation\"][[\"peak_id\", \"target_id\"]]\n",
    "peak_to_tg_edges = pd.merge(cicero_peak_to_tg, corr_peak_to_tg, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_to_tg_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65882f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_tf_to_peak = feature_score_dataframes[\"sliding_window_score\"][[\"source_id\", \"peak_id\"]]\n",
    "homer_tf_to_peak = feature_score_dataframes[\"homer_binding_score\"][[\"source_id\", \"peak_id\"]]\n",
    "tf_to_peak_edges = pd.merge(sliding_window_tf_to_peak, homer_tf_to_peak, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_peak_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_peak_tg_edges = pd.merge(tf_to_peak_edges, peak_to_tg_edges, on=[\"peak_id\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf745c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_peak_tg_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fefe30",
   "metadata": {},
   "source": [
    "Aggregating the TF-TG edges by the number of peaks between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a76dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edges = tf_peak_tg_edges.groupby(by=[\"source_id\", \"target_id\"]).size().reset_index(name=\"edge_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab9177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "def read_ground_truth(ground_truth_file):\n",
    "    ground_truth = pd.read_csv(ground_truth_file, sep='\\t', quoting=csv.QUOTE_NONE, on_bad_lines='skip', header=0)\n",
    "    ground_truth = ground_truth.rename(columns={\"Source\": \"source_id\", \"Target\": \"target_id\"})\n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "ground_truth_file = \"/gpfs/Labs/Uzun/DATA/PROJECTS/2024.SC_MO_TRN_DB.MIRA/REPOSITORY/CURRENT/REFERENCE_NETWORKS/RN111_ChIPSeq_BEELINE_Mouse_ESC.tsv\"\n",
    "ground_truth_df = read_ground_truth(ground_truth_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "inferred_network_df = dd.read_parquet(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/src/testing_scripts/tmp/tf_peak_tg_edges.parquet\", engine=\"pyarrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93078abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_edges_with_ground_truth(inferred_network_dd, ground_truth_df):\n",
    "    import dask.dataframe as dd\n",
    "    import numpy as np\n",
    "    ground_truth_pairs = set(zip(\n",
    "        ground_truth_df[\"source_id\"].str.upper(),\n",
    "        ground_truth_df[\"target_id\"].str.upper()\n",
    "    ))\n",
    "\n",
    "\n",
    "    def label_partition(df):\n",
    "        df = df.copy()  # <-- avoids SettingWithCopyWarning\n",
    "        tf_tg_tuples = list(zip(df[\"source_id\"], df[\"target_id\"]))\n",
    "        df.loc[:, \"label\"] = [1 if pair in ground_truth_pairs else 0 for pair in tf_tg_tuples]\n",
    "        return df\n",
    "\n",
    "    inferred_network_dd = inferred_network_dd.map_partitions(\n",
    "        label_partition,\n",
    "        meta=inferred_network_dd._meta.assign(label=np.int64(0))\n",
    "    )\n",
    "\n",
    "    return inferred_network_dd\n",
    "\n",
    "tf_tg_edges_in_gt = label_edges_with_ground_truth(inferred_network_df, ground_truth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d984a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edges_in_gt_df = tf_tg_edges_in_gt.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058db516",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edges_in_gt_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
