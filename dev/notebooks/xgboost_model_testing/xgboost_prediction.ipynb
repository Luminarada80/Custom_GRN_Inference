{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4512a-02da-46a1-92bb-78617ae07f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"Apply Dask-trained XGBoost model to a new network\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, required=True, help=\"Directory to save predictions\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to trained XGBoost .json Booster model\")\n",
    "    parser.add_argument(\"--target\", type=str, required=True, help=\"Path to .parquet file for inference\")\n",
    "    parser.add_argument(\"--save_name\", type=str, required=True, help=\"Filename for output\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def read_inferred_network(inferred_network_file: str) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a melted sparse inferred network from Parquet and pivots it into a Dask DataFrame\n",
    "    where each row is (source_id, target_id) and columns are score_types (mean-aggregated).\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading melted sparse network from: {inferred_network_file}\")\n",
    "    melted_ddf = dd.read_parquet(inferred_network_file, engine=\"pyarrow\")\n",
    "\n",
    "    # Standardize IDs\n",
    "    melted_ddf[\"source_id\"] = melted_ddf[\"source_id\"].str.upper()\n",
    "    melted_ddf[\"target_id\"] = melted_ddf[\"target_id\"].str.upper()\n",
    "\n",
    "    # Aggregate scores\n",
    "    grouped_ddf = (\n",
    "        melted_ddf\n",
    "        .groupby([\"source_id\", \"peak_id\", \"target_id\", \"score_type\"])[\"score_value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot manually by converting to pandas (if dataset is small enough)\n",
    "    def pivot_partition(df):\n",
    "        return df.pivot_table(\n",
    "            index=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "            columns=\"score_type\",\n",
    "            values=\"score_value\",\n",
    "            aggfunc=\"first\"\n",
    "        ).reset_index()\n",
    "\n",
    "    # Apply pivot in a single partition (best if you've already aggregated)\n",
    "    pivot_df = grouped_ddf.compute()  # convert to Pandas here\n",
    "    pivot_df = pivot_partition(pivot_df)\n",
    "    return dd.from_pandas(pivot_df, npartitions=1)\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    model_path = args.model\n",
    "    target_path = args.target\n",
    "    output_dir = args.output_dir\n",
    "    save_name = args.save_name\n",
    "\n",
    "    logging.info(\"Loading XGBoost Booster\")\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(model_path)\n",
    "\n",
    "    logging.info(\"Reading inferred network\")\n",
    "    inferred_dd = read_inferred_network(target_path)\n",
    "    \n",
    "    feature_names = booster.feature_names\n",
    "    \n",
    "    X_dd = inferred_dd[feature_names]\n",
    "\n",
    "    logging.info(\"Converting to DaskDMatrix\")\n",
    "    client = Client()\n",
    "    dtest = xgb.dask.DaskDMatrix(data=X_dd, feature_names=feature_names, client=client)\n",
    "\n",
    "    logging.info(\"Running distributed prediction\")\n",
    "    y_pred = xgb.dask.predict(client=client, model=booster, data=dtest)\n",
    "\n",
    "    # Convert to pandas (merging Dask DataFrame + Dask array)\n",
    "    logging.info(\"Joining predictions back to source-target pairs\")\n",
    "    result_df = inferred_dd[[\"source_id\", \"peak_id\", \"target_id\"]].compute()\n",
    "    result_df[\"score\"] = y_pred.compute()\n",
    "    result_df = result_df.drop_duplicates()\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    output_path = os.path.join(output_dir, save_name)\n",
    "    logging.info(f\"Saving to {output_path}\")\n",
    "    result_df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    logging.info(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "def read_inferred_network(inferred_network_file: str) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a melted sparse inferred network from Parquet and pivots it into a Dask DataFrame\n",
    "    where each row is (source_id, target_id) and columns are score_types (mean-aggregated).\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading melted sparse network from: {inferred_network_file}\")\n",
    "    melted_ddf = dd.read_parquet(inferred_network_file, engine=\"pyarrow\")\n",
    "\n",
    "    # Standardize IDs\n",
    "    melted_ddf[\"source_id\"] = melted_ddf[\"source_id\"].str.upper()\n",
    "    melted_ddf[\"target_id\"] = melted_ddf[\"target_id\"].str.upper()\n",
    "\n",
    "    # Aggregate scores\n",
    "    grouped_ddf = (\n",
    "        melted_ddf\n",
    "        .groupby([\"source_id\", \"peak_id\", \"target_id\", \"score_type\"])[\"score_value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot manually by converting to pandas (if dataset is small enough)\n",
    "    def pivot_partition(df):\n",
    "        return df.pivot_table(\n",
    "            index=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "            columns=\"score_type\",\n",
    "            values=\"score_value\",\n",
    "            aggfunc=\"first\"\n",
    "        ).reset_index()\n",
    "\n",
    "    # Apply pivot in a single partition (best if you've already aggregated)\n",
    "    pivot_df = grouped_ddf.compute()  # convert to Pandas here\n",
    "    pivot_df = pivot_partition(pivot_df)\n",
    "    return dd.from_pandas(pivot_df, npartitions=1)\n",
    "\n",
    "model_path = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1_old/trained_models/xgb_DS011_mESC_sample1_old_model.json\"\n",
    "target_path = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/mESC/filtered_L2_E7.5_rep2_old/inferred_grns/inferred_score_df.parquet\"\n",
    "output_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1_old/model_predictions\"\n",
    "save_name = \"testing_aggregation_predictions.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Loading XGBoost Booster\")\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Reading inferred network\")\n",
    "inferred_dd = read_inferred_network(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = booster.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dd = inferred_dd[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Converting to DaskDMatrix\")\n",
    "client = Client()\n",
    "dtest = xgb.dask.DaskDMatrix(data=X_dd, feature_names=feature_names, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Running distributed prediction\")\n",
    "y_pred = xgb.dask.predict(client=client, model=booster, data=dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas (merging Dask DataFrame + Dask array)\n",
    "logging.info(\"Joining predictions back to source-target pairs\")\n",
    "result_df = inferred_dd[[\"source_id\", \"peak_id\", \"target_id\"]].compute()\n",
    "result_df[\"score\"] = y_pred.compute()\n",
    "result_df = result_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
