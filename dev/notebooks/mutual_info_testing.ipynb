{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to my_env (Python 3.9.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1/ds011_nite_predictions.csv\", header=0, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_predictions = df.mean(axis=1)\n",
    "nite_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_predictions = df.mean(axis=1).rename_axis(\"NITE_score\")\n",
    "nite_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_predictions = df.mean(axis=1).rename(\"NITE_score\")\n",
    "nite_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_predictions = nite_predictions.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_predictions.to_parquet(\"output/DS011_mESC/DS011_mESC_sample1/ds011_nite_predictions.parquet\", engine=\"pyarrow\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira_df = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/ds011_peak_to_gene_lite_rp_score_chrom_diff.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_df = nite_predictions.rename_axis(\"target_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_df = nite_predictions.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_df = nite_df.rename(columns={\"index\": \"target_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nite_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mira_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(mira_df, nite_df, on=\"target_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.rename(columns={\"MIRA_LITE_RP_score\": \"LITE_score\"})\n",
    "merged_df = merged_df[[\"peak_id\", \"target_id\", \"LITE_score\", \"NITE_score\", \"chromatin_differential\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_parquet(\"output/DS011_mESC/DS011_mESC_sample1/ds011_peak_to_gene_lite_rp_score_chrom_diff.parquet\", engine=\"pyarrow\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/labeled_inferred_grn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [\n",
    "    'mean_tf_expr', 'mean_peak_expr', 'mean_tg_expr',\n",
    "    'cicero_score', 'TSS_dist_score', 'correlation',\n",
    "    'LITE_score', 'NITE_score', 'chromatin_differential', \n",
    "    'sliding_window_score', 'homer_binding_score',\n",
    "    ]\n",
    "\n",
    "\n",
    "corrs = score_df[score_cols + [\"label\"]].corr()[\"label\"].sort_values(ascending=False)\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "mi = mutual_info_classif(score_df[score_cols], score_df[\"label\"])\n",
    "print(pd.Series(mi, index=score_cols).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "# Subset relevant features and label\n",
    "cols = score_cols + ['label']\n",
    "df_subset = score_df[cols].copy()\n",
    "\n",
    "# Drop rows with any NaNs (safe for quick analysis)\n",
    "df_subset = df_subset.dropna()\n",
    "\n",
    "# Separate X (features) and y (labels)\n",
    "X = df_subset[score_cols]\n",
    "y = df_subset['label']\n",
    "\n",
    "# Compute mutual information\n",
    "mi = mutual_info_classif(X, y, discrete_features=False, random_state=42)\n",
    "mi_series = pd.Series(mi, index=score_cols).sort_values(ascending=False)\n",
    "\n",
    "print(mi_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/labeled_inferred_grn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "score_df = pd.read_parquet(\"output/DS011_mESC/DS011_mESC_sample1/labeled_inferred_grn.parquet\")\n",
    "\n",
    "score_cols = [\n",
    "    'mean_tf_expr', 'mean_peak_expr', 'mean_tg_expr',\n",
    "    'cicero_score', 'TSS_dist_score', 'correlation',\n",
    "    'LITE_score', 'NITE_score', 'chromatin_differential', \n",
    "    'sliding_window_score', 'homer_binding_score', 'string_experimental_score',\n",
    "    'string_textmining_score', 'string_combined_score'\n",
    "    ]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Iterate through each TF\n",
    "for tf, group in score_df.groupby(\"source_id\"):\n",
    "    X = group[score_cols].fillna(0).copy()\n",
    "    y = group[\"label\"].fillna(0).copy()\n",
    "\n",
    "    # Skip if too few samples\n",
    "    if len(X) < 1 or y.nunique() < 2:\n",
    "        continue\n",
    "\n",
    "    mi_scores = mutual_info_classif(X, y, random_state=42, discrete_features=False)\n",
    "    for feat, score in zip(score_cols, mi_scores):\n",
    "        results.append({\"TF\": tf, \"Feature\": feat, \"MI_score\": score})\n",
    "\n",
    "# Combine results\n",
    "mi_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "pivot = mi_df.pivot(index=\"TF\", columns=\"Feature\", values=\"MI_score\")\n",
    "plt.figure(figsize=(14, len(pivot) * 0.8))\n",
    "sns.heatmap(pivot, cmap=\"viridis\", linewidths=0)\n",
    "plt.title(\"Mutual Information per TF\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
