{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f4512a-02da-46a1-92bb-78617ae07f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "def parse_args() -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"Apply Dask-trained XGBoost model to a new network\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, required=True, help=\"Directory to save predictions\")\n",
    "    parser.add_argument(\"--model\", type=str, required=True, help=\"Path to trained XGBoost .json Booster model\")\n",
    "    parser.add_argument(\"--target\", type=str, required=True, help=\"Path to .parquet file for inference\")\n",
    "    parser.add_argument(\"--save_name\", type=str, required=True, help=\"Filename for output\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def read_inferred_network(inferred_network_file: str) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a melted sparse inferred network from Parquet and pivots it into a Dask DataFrame\n",
    "    where each row is (source_id, target_id) and columns are score_types (mean-aggregated).\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading melted sparse network from: {inferred_network_file}\")\n",
    "    melted_ddf = dd.read_parquet(inferred_network_file, engine=\"pyarrow\")\n",
    "\n",
    "    # Standardize IDs\n",
    "    melted_ddf[\"source_id\"] = melted_ddf[\"source_id\"].str.upper()\n",
    "    melted_ddf[\"target_id\"] = melted_ddf[\"target_id\"].str.upper()\n",
    "\n",
    "    # Aggregate scores\n",
    "    grouped_ddf = (\n",
    "        melted_ddf\n",
    "        .groupby([\"source_id\", \"peak_id\", \"target_id\", \"score_type\"])[\"score_value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot manually by converting to pandas (if dataset is small enough)\n",
    "    def pivot_partition(df):\n",
    "        return df.pivot_table(\n",
    "            index=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "            columns=\"score_type\",\n",
    "            values=\"score_value\",\n",
    "            aggfunc=\"first\"\n",
    "        ).reset_index()\n",
    "\n",
    "    # Apply pivot in a single partition (best if you've already aggregated)\n",
    "    pivot_df = grouped_ddf.compute()  # convert to Pandas here\n",
    "    pivot_df = pivot_partition(pivot_df)\n",
    "    return dd.from_pandas(pivot_df, npartitions=1)\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    model_path = args.model\n",
    "    target_path = args.target\n",
    "    output_dir = args.output_dir\n",
    "    save_name = args.save_name\n",
    "\n",
    "    logging.info(\"Loading XGBoost Booster\")\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(model_path)\n",
    "\n",
    "    logging.info(\"Reading inferred network\")\n",
    "    inferred_dd = read_inferred_network(target_path)\n",
    "    \n",
    "    feature_names = booster.feature_names\n",
    "    \n",
    "    X_dd = inferred_dd[feature_names]\n",
    "\n",
    "    logging.info(\"Converting to DaskDMatrix\")\n",
    "    client = Client()\n",
    "    dtest = xgb.dask.DaskDMatrix(data=X_dd, feature_names=feature_names, client=client)\n",
    "\n",
    "    logging.info(\"Running distributed prediction\")\n",
    "    y_pred = xgb.dask.predict(client=client, model=booster, data=dtest)\n",
    "\n",
    "    # Convert to pandas (merging Dask DataFrame + Dask array)\n",
    "    logging.info(\"Joining predictions back to source-target pairs\")\n",
    "    result_df = inferred_dd[[\"source_id\", \"peak_id\", \"target_id\"]].compute()\n",
    "    result_df[\"score\"] = y_pred.compute()\n",
    "    result_df = result_df.drop_duplicates()\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    output_path = os.path.join(output_dir, save_name)\n",
    "    logging.info(f\"Saving to {output_path}\")\n",
    "    result_df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    logging.info(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "def read_inferred_network(inferred_network_file: str) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a melted sparse inferred network from Parquet and pivots it into a Dask DataFrame\n",
    "    where each row is (source_id, target_id) and columns are score_types (mean-aggregated).\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading melted sparse network from: {inferred_network_file}\")\n",
    "    melted_ddf = dd.read_parquet(inferred_network_file, engine=\"pyarrow\")\n",
    "\n",
    "    # Standardize IDs\n",
    "    melted_ddf[\"source_id\"] = melted_ddf[\"source_id\"].str.upper()\n",
    "    melted_ddf[\"target_id\"] = melted_ddf[\"target_id\"].str.upper()\n",
    "\n",
    "    # Aggregate scores\n",
    "    grouped_ddf = (\n",
    "        melted_ddf\n",
    "        .groupby([\"source_id\", \"peak_id\", \"target_id\", \"score_type\"])[\"score_value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot manually by converting to pandas (if dataset is small enough)\n",
    "    def pivot_partition(df):\n",
    "        return df.pivot_table(\n",
    "            index=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "            columns=\"score_type\",\n",
    "            values=\"score_value\",\n",
    "            aggfunc=\"first\"\n",
    "        ).reset_index()\n",
    "\n",
    "    # Apply pivot in a single partition (best if you've already aggregated)\n",
    "    pivot_df = grouped_ddf.compute()  # convert to Pandas here\n",
    "    pivot_df = pivot_partition(pivot_df)\n",
    "    return dd.from_pandas(pivot_df, npartitions=1)\n",
    "\n",
    "model_path = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1_old/trained_models/xgb_DS011_mESC_sample1_old_model.json\"\n",
    "target_path = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/mESC/filtered_L2_E7.5_rep2_old/inferred_grns/inferred_score_df.parquet\"\n",
    "output_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/DS011_mESC/DS011_mESC_sample1_old/model_predictions\"\n",
    "save_name = \"testing_aggregation_predictions.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Loading XGBoost Booster\")\n",
    "booster = xgb.Booster()\n",
    "booster.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Reading inferred network\")\n",
    "inferred_dd = read_inferred_network(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = booster.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dd = inferred_dd[feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Converting to DaskDMatrix\")\n",
    "client = Client()\n",
    "dtest = xgb.dask.DaskDMatrix(data=X_dd, feature_names=feature_names, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Running distributed prediction\")\n",
    "y_pred = xgb.dask.predict(client=client, model=booster, data=dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas (merging Dask DataFrame + Dask array)\n",
    "logging.info(\"Joining predictions back to source-target pairs\")\n",
    "result_df = inferred_dd[[\"source_id\", \"peak_id\", \"target_id\"]].compute()\n",
    "result_df[\"score\"] = y_pred.compute()\n",
    "result_df = result_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edge_groups = result_df.groupby([\"source_id\", \"target_id\"])[\"peak_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edge_mean_peak_scores = tf_tg_edge_groups.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edge_mean_peak_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edge_scores = result_df.groupby([\"source_id\", \"target_id\"])[\"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores = tf_tg_edge_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_ground_truth(ground_truth_file):\n",
    "    ground_truth = pd.read_csv(ground_truth_file, sep='\\t', quoting=csv.QUOTE_NONE, on_bad_lines='skip', header=0)\n",
    "    ground_truth = ground_truth.rename(columns={\"Source\": \"source_id\", \"Target\": \"target_id\"})\n",
    "    return ground_truth\n",
    "\n",
    "def label_edges_with_ground_truth(inferred_network_dd, ground_truth_df):\n",
    "    import dask.dataframe as dd\n",
    "    import numpy as np\n",
    "    ground_truth_pairs = set(zip(\n",
    "        ground_truth_df[\"source_id\"].str.upper(),\n",
    "        ground_truth_df[\"target_id\"].str.upper()\n",
    "    ))\n",
    "    \n",
    "    inferred_network_dd[\"source_id\"] = inferred_network_dd[\"source_id\"].str.upper()\n",
    "    inferred_network_dd[\"target_id\"] = inferred_network_dd[\"target_id\"].str.upper()\n",
    "\n",
    "\n",
    "    def label_partition(df):\n",
    "        df = df.copy()\n",
    "        tf_tg_tuples = list(zip(df[\"source_id\"], df[\"target_id\"]))\n",
    "        df.loc[:, \"label\"] = [1 if pair in ground_truth_pairs else 0 for pair in tf_tg_tuples]\n",
    "        return df\n",
    "\n",
    "    inferred_network_dd = inferred_network_dd.map_partitions(\n",
    "        label_partition,\n",
    "        meta=inferred_network_dd._meta.assign(label=np.int64(0))\n",
    "    )\n",
    "\n",
    "    return inferred_network_dd\n",
    "\n",
    "ground_truth_file = \"/gpfs/Labs/Uzun/DATA/PROJECTS/2024.SC_MO_TRN_DB.MIRA/REPOSITORY/CURRENT/REFERENCE_NETWORKS/RN111_ChIPSeq_BEELINE_Mouse_ESC.tsv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = read_ground_truth(ground_truth_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_edges_with_ground_truth(inferred_network_dd, ground_truth_df):\n",
    "    import dask.dataframe as dd\n",
    "    import numpy as np\n",
    "    ground_truth_pairs = set(zip(\n",
    "        ground_truth_df[\"source_id\"].str.upper(),\n",
    "        ground_truth_df[\"target_id\"].str.upper()\n",
    "    ))\n",
    "    \n",
    "    inferred_network_dd[\"source_id\"] = inferred_network_dd[\"source_id\"].str.upper()\n",
    "    inferred_network_dd[\"target_id\"] = inferred_network_dd[\"target_id\"].str.upper()\n",
    "\n",
    "\n",
    "    def label_partition(df):\n",
    "        df = df.copy()\n",
    "        tf_tg_tuples = list(zip(df[\"source_id\"], df[\"target_id\"]))\n",
    "        df.loc[:, \"label\"] = [1 if pair in ground_truth_pairs else 0 for pair in tf_tg_tuples]\n",
    "        return df\n",
    "\n",
    "    inferred_network_dd = inferred_network_dd.apply(\n",
    "        label_partition,\n",
    "    )\n",
    "\n",
    "    return inferred_network_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores = tf_tg_edge_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores = tf_tg_mean_scores.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_pairs = set(zip(\n",
    "    ground_truth_df[\"source_id\"].str.upper(),\n",
    "    ground_truth_df[\"target_id\"].str.upper()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores[\"source_id\"] = tf_tg_mean_scores[\"source_id\"].str.upper()\n",
    "tf_tg_mean_scores[\"target_id\"] = tf_tg_mean_scores[\"target_id\"].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores[\"label\"] = [1 if pair in ground_truth_pairs else 0 for pair in list(zip(tf_tg_mean_scores[\"source_id\"], tf_tg_mean_scores[\"target_id\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_edges = tf_tg_mean_scores[tf_tg_mean_scores[\"label\"] == 1]\n",
    "false_edges = tf_tg_mean_scores[tf_tg_mean_scores[\"label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_true_false_to_string(value: Union[int, str]):\n",
    "    if type(value) == int:\n",
    "        if value == 0:\n",
    "            return \"False\"\n",
    "        elif value == 1:\n",
    "            return \"True\"\n",
    "        else:\n",
    "            raise ValueError(f\"Value {value} in the ground truth label columns is not 0 or 1\")\n",
    "    elif type(value) == str:\n",
    "        return value\n",
    "    else:\n",
    "        raise TypeError(\"Value must be either of type int or str, got %s\" % value)\n",
    "        \n",
    "\n",
    "tf_tg_mean_scores[\"label\"] = tf_tg_mean_scores[\"label\"].apply(convert_true_false_to_string)\n",
    "tf_tg_pivoted: pd.DataFrame = tf_tg_mean_scores.pivot(columns=\"label\", values=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true = len(tf_tg_mean_scores[tf_tg_mean_scores[\"label\"] == \"True\"])\n",
    "n_false = len(tf_tg_mean_scores[tf_tg_mean_scores[\"label\"] == \"False\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_group_count = min(n_false, n_true)\n",
    "\n",
    "tf_tg_true = tf_tg_mean_scores[tf_tg_mean_scores[\"label\"] == \"True\"].sample(min_group_count)\n",
    "tf_tg_false = tf_tg_mean_scores[tf_tg_mean_scores[\"label\"] == \"False\"].sample(min_group_count)\n",
    "\n",
    "tf_tg_balanced = pd.concat([tf_tg_true, tf_tg_false])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tf_tg_balanced[tf_tg_balanced[\"label\"] == \"False\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tf_tg_balanced[tf_tg_balanced[\"label\"] == \"True\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.histplot(data=tf_tg_balanced, x=\"score\", hue=\"label\", bins=50, element=\"step\", stat=\"count\")\n",
    "plt.title(\"Distribution of peak counts for TF-TG edges\", fontsize=16)\n",
    "plt.xlabel(\"Number of peaks\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edge_scores.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edge_scores.count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tg_edge_count = tf_tg_edge_scores.count().reset_index()\n",
    "tf_tg_edge_count[\"label\"] = [\"True\" if pair in ground_truth_pairs else \"False\" for pair in list(zip(tf_tg_edge_count[\"source_id\"], tf_tg_edge_count[\"target_id\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true = len(tf_tg_edge_count[tf_tg_edge_count[\"label\"] == \"True\"])\n",
    "n_false = len(tf_tg_edge_count[tf_tg_edge_count[\"label\"] == \"False\"])\n",
    "\n",
    "min_group_count = min(n_false, n_true)\n",
    "\n",
    "tf_tg_true = tf_tg_edge_count[tf_tg_edge_count[\"label\"] == \"True\"].sample(min_group_count)\n",
    "tf_tg_false = tf_tg_edge_count[tf_tg_edge_count[\"label\"] == \"False\"].sample(min_group_count)\n",
    "\n",
    "tf_tg_count_balanced = pd.concat([tf_tg_true, tf_tg_false])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.histplot(data=tf_tg_count_balanced, x=\"score\", hue=\"label\", bins=50, element=\"step\", stat=\"count\")\n",
    "plt.title(\"Distribution of peak counts for TF-TG edges\", fontsize=16)\n",
    "plt.xlabel(\"Number of peaks\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.histplot(data=tf_tg_count_balanced, x=\"score\", hue=\"label\", log_scale=True, bins=50, element=\"step\", stat=\"count\")\n",
    "plt.title(\"Distribution of peak counts for TF-TG edges\", fontsize=16)\n",
    "plt.xlabel(\"Number of peaks\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df = pd.read_parquet(target_path, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate scores\n",
    "grouped_ddf = (\n",
    "    inferred_df\n",
    "    .groupby([\"source_id\", \"peak_id\", \"target_id\", \"score_type\"])[\"score_value\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Pivot manually by converting to pandas (if dataset is small enough)\n",
    "def pivot_partition(df):\n",
    "    return df.pivot_table(\n",
    "        index=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "        columns=\"score_type\",\n",
    "        values=\"score_value\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "inferred_df_full = pivot_partition(grouped_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_tf_tg_score(peak_scores):\n",
    "    return np.percentile(peak_scores, 75)\n",
    "\n",
    "def softmax_weighted_average(peak_scores, lambda_=5.0):\n",
    "    weights = np.exp(lambda_ * peak_scores)\n",
    "    return np.sum(weights * peak_scores) / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "agg_df = result_df.groupby([\"source_id\", \"target_id\"])[\"score\"].agg(\n",
    "    tf_tg_score_75pct = lambda x: np.percentile(x, 75),\n",
    "    tf_tg_score_softmax = lambda x: softmax_weighted_average(x, lambda_=5.0),\n",
    "    tf_tg_frac_high = lambda x: np.mean(x > threshold)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df[\"label\"] = [\"True\" if pair in ground_truth_pairs else \"False\" for pair in list(zip(agg_df[\"source_id\"], agg_df[\"target_id\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true = len(agg_df[agg_df[\"label\"] == \"True\"])\n",
    "n_false = len(agg_df[agg_df[\"label\"] == \"False\"])\n",
    "\n",
    "min_group_count = min(n_false, n_true)\n",
    "\n",
    "tf_tg_true = agg_df[agg_df[\"label\"] == \"True\"].sample(min_group_count)\n",
    "tf_tg_false = agg_df[agg_df[\"label\"] == \"False\"].sample(min_group_count)\n",
    "\n",
    "tf_tg_count_balanced = pd.concat([tf_tg_true, tf_tg_false])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
