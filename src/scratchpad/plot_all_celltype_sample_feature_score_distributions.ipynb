{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['filtered_L2_E7.5_rep1', 'filtered_L2_E7.5_rep2', 'filtered_L2_E7.75_rep1', 'filtered_L2_E8.0_rep1', 'filtered_L2_E8.0_rep2', 'filtered_L2_E8.5_rep1', 'filtered_L2_E8.5_rep2', 'filtered_L2_E8.75_rep2']\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "mesc_output_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/output/mESC\"\n",
    "\n",
    "parquet_paths = sorted(glob.glob(f\"{mesc_output_dir}/*/inferred_grns/inferred_score_df.parquet\"))\n",
    "\n",
    "# go up two levels: parquet → inferred_grns dir → sample dir\n",
    "network_name_list = [\n",
    "    os.path.basename(os.path.dirname(os.path.dirname(p)))\n",
    "    for p in parquet_paths\n",
    "]\n",
    "\n",
    "print(network_name_list)\n",
    "\n",
    "# 4) Define your bin edges\n",
    "n_bins = 50\n",
    "bins = np.linspace(0,1,51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inferred_network_dask(inferred_network_file: str) -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a melted sparse inferred network from Parquet and pivots it into a Dask DataFrame\n",
    "    where each row is (source_id, target_id) and columns are score_types (mean-aggregated).\n",
    "    \"\"\"\n",
    "    melted_ddf: dd.DataFrame = dd.read_parquet(inferred_network_file, engine=\"pyarrow\")\n",
    "\n",
    "    # Standardize IDs\n",
    "    melted_ddf[\"source_id\"] = melted_ddf[\"source_id\"].str.upper()\n",
    "    melted_ddf[\"target_id\"] = melted_ddf[\"target_id\"].str.upper()\n",
    "\n",
    "    # Aggregate scores\n",
    "    grouped_ddf = (\n",
    "        melted_ddf\n",
    "        .groupby([\"source_id\", \"target_id\", \"score_type\"])[\"score_value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot manually by converting to pandas (if dataset is small enough)\n",
    "    def pivot_partition(df):\n",
    "        return df.pivot_table(\n",
    "            index=[\"source_id\", \"target_id\"],\n",
    "            columns=\"score_type\",\n",
    "            values=\"score_value\",\n",
    "            aggfunc=\"mean\"\n",
    "        ).reset_index()\n",
    "\n",
    "    # Apply pivot in a single partition (best if you've already aggregated)\n",
    "    pivot_df = grouped_ddf.compute()  # convert to Pandas here\n",
    "    pivot_df = pivot_partition(pivot_df)\n",
    "    return dd.from_pandas(pivot_df, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_plot_histograms(\n",
    "    sample_name,\n",
    "    bins,\n",
    "    features,\n",
    "    axes,\n",
    "    mesc_output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads one network, computes per-feature histogram proportions\n",
    "    using the fixed 'bins', and plots onto the provided axes.\n",
    "    \"\"\"\n",
    "    # 1) Load & pivot\n",
    "    path = os.path.join(\n",
    "        mesc_output_dir,\n",
    "        sample_name, \"inferred_grns\", \"inferred_score_df.parquet\"\n",
    "    )\n",
    "    ddf = read_inferred_network_dask(path)\n",
    "    df  = ddf.compute()\n",
    "\n",
    "    # 2) For each feature, compute and plot\n",
    "    x_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    for j, feat in enumerate(features):\n",
    "        counts, _ = np.histogram(df[feat].dropna(), bins=bins)\n",
    "        prop = counts / counts.sum()\n",
    "        ax = axes.flat[j]\n",
    "        ax.plot(x_centers, prop, label=sample_name, alpha=0.7)\n",
    "        ax.set_title(feat, fontsize=14)\n",
    "        ax.set_xlabel(feat, fontsize=14)\n",
    "        ax.set_ylabel(\"Proportion\", fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['TSS_dist_score', 'cicero_score', 'correlation', 'homer_binding_score', 'mean_TF_expression', 'mean_TG_expression', 'mean_peak_accessibility', 'sliding_window_score', 'string_combined_score', 'string_experimental_score', 'string_textmining_score']\n",
    "\n",
    "ncols = 4\n",
    "nrows = math.ceil(len(features) / ncols)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 4*nrows), squeeze=False)\n",
    "\n",
    "# One pass per sample, plotting onto the shared axes grid\n",
    "for sample_name in network_name_list:\n",
    "    print(f'Plotting for {sample_name}')\n",
    "    compute_and_plot_histograms(\n",
    "        sample_name,\n",
    "        bins,\n",
    "        features,\n",
    "        axes,\n",
    "        mesc_output_dir\n",
    "    )\n",
    "\n",
    "# Common legend and layout\n",
    "handles, labels = axes.flat[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels,\n",
    "           loc=\"lower center\", ncol=3, fontsize=12,\n",
    "           bbox_to_anchor=(0.5, -0.02))\n",
    "fig.tight_layout(rect=[0, 0.05, 1, 1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
