{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e69874",
   "metadata": {},
   "source": [
    "# Exploring Score DataFrame Sizes\n",
    "\n",
    "As we combine the output DataFrames from multiple scoring methods, it is useful to know the sizes of each dataframe and how much overlap they have with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7230d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from grn_inference.normalization import minmax_normalize_pandas\n",
    "\n",
    "project_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER\"\n",
    "input_dir = os.path.join(project_dir, \"input/DS011_mESC/DS011_mESC_sample1\")\n",
    "output_dir = os.path.join(project_dir, \"output/DS011_mESC/DS011_mESC_sample1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e64604",
   "metadata": {},
   "source": [
    "## Size of the Input Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa378019",
   "metadata": {},
   "outputs": [],
   "source": [
    "atac_processed_df = pd.read_parquet(\n",
    "    os.path.join(input_dir, \"DS011_mESC_ATAC_processed.parquet\"), \n",
    "    engine=\"pyarrow\"\n",
    "    )\n",
    "\n",
    "rna_processed_df = pd.read_parquet(\n",
    "    os.path.join(input_dir, \"DS011_mESC_RNA_processed.parquet\"), \n",
    "    engine=\"pyarrow\"\n",
    "    )\n",
    "\n",
    "print(\"scATAC-seq Dataset:\")\n",
    "print(f\"  - Cells: {atac_processed_df.shape[1]-1}\")\n",
    "print(f\"  - Peaks: {atac_processed_df.shape[0]}\")\n",
    "\n",
    "print(\"\\nscRNA-seq Dataset:\")\n",
    "print(f\"  - Cells: {rna_processed_df.shape[1]-1}\")\n",
    "print(f\"  - Genes: {rna_processed_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afcadbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc442f",
   "metadata": {},
   "source": [
    "## Peak to TG\n",
    "\n",
    "The peak to TG regulatory potential methods we use are:\n",
    "\n",
    "1) Cicero\n",
    "2) TSS Distance\n",
    "3) MIRA\n",
    "4) TG Expression / Peak Accessibility correlation\n",
    "\n",
    "First, let's look at the size of each methods output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a72da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_to_tg_score_summary(score_file_name, method_name) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(output_dir, score_file_name)\n",
    "    )\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\")\n",
    "    print(f\"  - Unique TGs: {df['target_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "method_files = {\n",
    "    \"Cicero\": \"cicero_peak_to_tg_scores.parquet\",\n",
    "    \"TSS Distance Score\": \"tss_distance_score.parquet\",\n",
    "    \"MIRA\": \"ds011_peak_to_gene_lite_rp_score_chrom_diff.parquet\",\n",
    "    \"Correlation\": \"peak_to_gene_correlation.parquet\"\n",
    "}\n",
    "\n",
    "method_dfs = {\n",
    "    name: peak_to_tg_score_summary(path, name)\n",
    "    for name, path in method_files.items()\n",
    "}\n",
    "\n",
    "method_peaks = {name: set(df[\"peak_id\"]) for name, df in method_dfs.items()}\n",
    "method_targets = {name: set(df[\"target_id\"]) for name, df in method_dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d80d2c",
   "metadata": {},
   "source": [
    "The TSS distance score DataFrame is much larger than the others. Let's look at how many of the peak to TG edges are shared between scoring methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1535c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shared_elements(method_dict, label):\n",
    "    shared_elements = set()\n",
    "    method_names = list(method_dict.keys())\n",
    "    print(f\"\\nShared {label}s:\")\n",
    "    for i, m1 in enumerate(method_names):\n",
    "        for m2 in method_names[i+1:]:\n",
    "            shared = method_dict[m1] & method_dict[m2]\n",
    "            print(f\"  - {m1} vs {m2}: {len(shared):,}\")\n",
    "            shared_elements.update(shared)\n",
    "    return shared_elements\n",
    "\n",
    "# Compute shared peaks and target genes\n",
    "shared_peaks = get_shared_elements(method_peaks, \"Peak\")\n",
    "shared_targets = get_shared_elements(method_targets, \"Target Gene\")\n",
    "\n",
    "print(f\"\\nPeaks in more than one method: {len(shared_peaks):,}\")\n",
    "print(f\"Target genes in more than one method: {len(shared_targets):,}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64db4a3",
   "metadata": {},
   "source": [
    "There is not a ton of overlap, but this reduces the size of the DataFrame. Now, lets subset each score DataFrame to only use edges that are in more than one scoring method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd6814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_df_to_peaks_and_targets_in_more_than_one_df(df, method_name, shared_peaks, shared_targets) -> pd.DataFrame:\n",
    "    df = df[\n",
    "    (df[\"peak_id\"].isin(shared_peaks)) &\n",
    "    (df[\"target_id\"].isin(shared_targets))\n",
    "    ].dropna()\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\")\n",
    "    print(f\"  - Unique TGs: {df['target_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Filter each dataframe to only those peak-target pairs that are in multiple methods\n",
    "method_dfs_subset = {\n",
    "    name: subset_df_to_peaks_and_targets_in_more_than_one_df(df, name, shared_peaks, shared_targets)\n",
    "    for name, df in method_dfs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d47707",
   "metadata": {},
   "source": [
    "Once we have only edges found in more than one score DataFrame, we can merge the scoring methods together into a single tf to peak DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3984c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_scores_no_tss = pd.merge(\n",
    "    pd.merge(method_dfs_subset[\"Cicero\"], method_dfs_subset[\"MIRA\"], on=[\"peak_id\", \"target_id\"], how=\"outer\"),\n",
    "    method_dfs_subset[\"Correlation\"], on=[\"peak_id\", \"target_id\"], how=\"inner\"\n",
    ")\n",
    "\n",
    "peak_to_tg = pd.merge(\n",
    "    merge_scores_no_tss, \n",
    "    method_dfs_subset[\"TSS Distance Score\"], \n",
    "    on=[\"peak_id\", \"target_id\"], \n",
    "    how=\"inner\"\n",
    "    )\n",
    "peak_to_tg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a414ab",
   "metadata": {},
   "source": [
    "Let's look at how sparse the dataset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_edges = len(peak_to_tg[[\"peak_id\", \"target_id\"]])\n",
    "score_cols = {\n",
    "    \"Cicero Score\": \"cicero_score\",\n",
    "    \"TSS Distance Score\": \"TSS_dist_score\",\n",
    "    \"LITE Score\": \"LITE_score\",\n",
    "    \"NITE Score\": \"NITE_score\",\n",
    "    \"MIRA Chromatin Differential Score\": \"chromatin_differential\",\n",
    "    \"Correlation Score\": \"correlation\"\n",
    "}\n",
    "\n",
    "print(f\"Total Edges: {total_edges:,}\")\n",
    "for label, col in score_cols.items():\n",
    "    score_count = peak_to_tg[col].notna().sum()\n",
    "    print(f\"  - {label}: {score_count:,} ({score_count / total_edges:.2%} of total scores)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373f404",
   "metadata": {},
   "source": [
    "The merged dataset isn't too sparse if we outer merge the method score DataFrames, but use an inner merge for the correlation and TSS distance scores. The number of peak to TG edges has dropped dramatically, which may or may not be a good thing. We can add a column called `\"support_count\"` which counts the number methods with non-nan values across rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b374869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many scores are non-null for each edge\n",
    "score_support_counts = peak_to_tg[score_cols.values()].notna().sum(axis=1)\n",
    "\n",
    "# Select edges supported by 2 or more methods\n",
    "for i, _ in enumerate(score_cols.values()):\n",
    "    if i > 0:\n",
    "        multi_supported_edges = peak_to_tg[score_support_counts >= i]\n",
    "\n",
    "        print(f\"Edges supported by â‰¥{i} methods: {len(multi_supported_edges):,} out of {len(peak_to_tg):,}\")\n",
    "peak_to_tg[\"support_count\"] = score_support_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750eb66",
   "metadata": {},
   "source": [
    "Now that we have a DataFrame containing all of the peak to TG scores for at least 2 methods, we can add in the **minmax normalized mean peak accessibility and mean TG expression** for each edge. We use an inner join here to only keep rows that have genes and peaks in the expression and accesibility data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc718024",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_peak_expr_norm = minmax_normalize_pandas(\n",
    "    atac_processed_df.set_index(\"peak_id\").mean(axis=1).rename(\"mean_peak_expr\").to_frame(),\n",
    "    [\"mean_peak_expr\"]\n",
    ").reset_index()\n",
    "\n",
    "mean_gene_expr_norm = minmax_normalize_pandas(\n",
    "    rna_processed_df.set_index(\"gene_id\").mean(axis=1).rename(\"mean_gene_expr\").to_frame(),\n",
    "    [\"mean_gene_expr\"]\n",
    ").reset_index()\n",
    "\n",
    "peak_to_tg_mean_acc = pd.merge(peak_to_tg, mean_peak_expr_norm, on=\"peak_id\", how=\"inner\")\n",
    "full_peak_to_tg = pd.merge(\n",
    "    peak_to_tg_mean_acc, mean_gene_expr_norm, left_on=\"target_id\", \n",
    "    right_on=\"gene_id\", how=\"inner\"\n",
    "    ).drop(columns=\"gene_id\").rename(columns={\"mean_gene_expr\":\"mean_tg_expr\"})\n",
    "full_peak_to_tg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74063ed8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3512676",
   "metadata": {},
   "source": [
    "## TF to Peak\n",
    "\n",
    "Now that we have the peak to TG scores together, we need to combine the TF to peak binding potential scores. We have two methods that calculate TF to peak scores:\n",
    "\n",
    "1) Homer\n",
    "2) Sliding Window\n",
    "\n",
    "Let's start by loading these score DataFrames in and take a look at their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5587b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_to_peak_score_summary(score_file_name, method_name) -> pd.DataFrame:\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(output_dir, score_file_name)\n",
    "    )\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  - Edges: {df.shape[0]:,}\")\n",
    "    print(f\"  - Unique TFs: {df['source_id'].nunique():,}\")\n",
    "    print(f\"  - Unique Peaks: {df['peak_id'].nunique():,}\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "method_files = {\n",
    "    \"Homer\": \"homer_tf_to_peak.parquet\",\n",
    "    \"Sliding Window\": \"sliding_window_tf_to_peak_score.parquet\"\n",
    "}\n",
    "\n",
    "method_dfs = {\n",
    "    name: tf_to_peak_score_summary(path, name)\n",
    "    for name, path in method_files.items()\n",
    "}\n",
    "\n",
    "method_TFs = {name: set(df[\"source_id\"]) for name, df in method_dfs.items()}\n",
    "method_peaks = {name: set(df[\"peak_id\"]) for name, df in method_dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe5265a",
   "metadata": {},
   "source": [
    "We will merge only on shared peaks and TFs between the two due to the large number of edges. We will also merge in the mean gene expression for the TFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_df = method_dfs[\"Sliding Window\"]\n",
    "homer_df = method_dfs[\"Homer\"]\n",
    "\n",
    "tf_to_peak_df = pd.merge(\n",
    "    pd.merge(sliding_window_df, homer_df, on=[\"peak_id\", \"source_id\"], how=\"inner\"),\n",
    "    mean_gene_expr_norm, left_on=\"source_id\", right_on=\"gene_id\", how=\"inner\"\n",
    "    ).drop(columns=\"gene_id\").rename(columns={\"mean_gene_expr\":\"mean_tf_expr\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95985ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_peak_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df = pd.merge(full_peak_to_tg, tf_to_peak_df, on=\"peak_id\", how=\"right\").dropna(subset=[\"source_id\", \"peak_id\", \"target_id\"])\n",
    "tf_to_tg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9756477",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df = tf_to_tg_df[[\n",
    "    'source_id', 'peak_id', 'target_id', \n",
    "    'mean_tf_expr', 'mean_peak_expr', 'mean_tg_expr',\n",
    "    'cicero_score', 'TSS_dist_score', 'correlation',\n",
    "    'LITE_score', 'NITE_score', 'chromatin_differential', \n",
    "    'sliding_window_score', 'homer_binding_score', 'support_count'\n",
    "    ]]\n",
    "tf_to_tg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df = minmax_normalize_pandas(tf_to_tg_df, [\"NITE_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_to_tg_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eb6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe to get it in the same format as combine_dataframes.py\n",
    "score_cols = [\n",
    "    'mean_tf_expr', 'mean_peak_expr', 'mean_tg_expr',\n",
    "    'cicero_score', 'TSS_dist_score', 'correlation',\n",
    "    'LITE_score', 'NITE_score', 'chromatin_differential', \n",
    "    'sliding_window_score', 'homer_binding_score', 'support_count'\n",
    "    ]\n",
    "\n",
    "melted_df = tf_to_tg_df.melt(\n",
    "    id_vars=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "    value_vars=score_cols,\n",
    "    var_name=\"score_type\",\n",
    "    value_name=\"score_value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eef0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df.to_parquet(os.path.join(output_dir, \"inferred_grns\", \"inferred_score_df.parquet\"), engine=\"pyarrow\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a24cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_to_tg_df[\"chromatin_differential\"].hist(bins=50, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab4a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mira_df = method_dfs[\"MIRA\"]\n",
    "mira_df[\"chromatin_differential\"].hist(bins=50, grid=False, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d095989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grn_inference.normalization import clip_and_normalize_log1p_pandas, minmax_normalize_pandas\n",
    "\n",
    "def add_string_db_scores(inferred_net_df):\n",
    "    string_dir = \"/gpfs/Labs/Uzun/SCRIPTS/PROJECTS/2024.SINGLE_CELL_GRN_INFERENCE.MOELLER/data/string_database/mm10\"\n",
    "    \n",
    "    # Load STRING protein info and links (small)\n",
    "    print(\"  - Reading STRING protein info\")\n",
    "    protein_info_df = pd.read_csv(f\"{string_dir}/protein_info.txt\", sep=\"\\t\")\n",
    "    \n",
    "    print(\"  - Reading STRING protein links detailed\")\n",
    "    protein_links_df = pd.read_csv(f\"{string_dir}/protein_links_detailed.txt\", sep=\" \")\n",
    "\n",
    "    print(\"  - Mapping STRING protein IDs to preferred names\")\n",
    "    id_to_name = protein_info_df.set_index(\"#string_protein_id\")[\"preferred_name\"].to_dict()\n",
    "    protein_links_df[\"protein1\"] = protein_links_df[\"protein1\"].map(id_to_name)\n",
    "    protein_links_df[\"protein2\"] = protein_links_df[\"protein2\"].map(id_to_name)\n",
    "    \n",
    "    inferred_net_df[\"source_id\"] = inferred_net_df[\"source_id\"].str.strip().str.upper()\n",
    "    inferred_net_df[\"target_id\"] = inferred_net_df[\"target_id\"].str.strip().str.upper()\n",
    "    protein_links_df[\"protein1\"] = protein_links_df[\"protein1\"].str.strip().str.upper()\n",
    "    protein_links_df[\"protein2\"] = protein_links_df[\"protein2\"].str.strip().str.upper()\n",
    "\n",
    "    id_to_name = {\n",
    "        k: v.strip().upper() for k, v in \n",
    "        protein_info_df.set_index(\"#string_protein_id\")[\"preferred_name\"].to_dict().items()\n",
    "    }\n",
    "\n",
    "    # Bi-directional match\n",
    "    tf_tg_pairs = set(zip(inferred_net_df[\"source_id\"], inferred_net_df[\"target_id\"]))\n",
    "    tf_tg_pairs |= set((tg, tf) for tf, tg in tf_tg_pairs)  # include reverse\n",
    "    \n",
    "    print(f\"  - Filtering STRING links to match {len(tf_tg_pairs):,} TFâ€“TG pairs\")\n",
    "    filtered_links_df = protein_links_df[\n",
    "        protein_links_df[[\"protein1\", \"protein2\"]]\n",
    "        .apply(tuple, axis=1)\n",
    "        .isin(tf_tg_pairs)\n",
    "    ]\n",
    "\n",
    "    # Rename columns and normalize\n",
    "    filtered_links_df = filtered_links_df.rename(columns={\n",
    "        \"experimental\": \"string_experimental_score\",\n",
    "        \"textmining\": \"string_textmining_score\",\n",
    "        \"combined_score\": \"string_combined_score\"\n",
    "    })[[\"protein1\", \"protein2\", \"string_experimental_score\", \"string_textmining_score\", \"string_combined_score\"]]\n",
    "\n",
    "    # filtered_links_df = clip_and_normalize_log1p_pandas(\n",
    "    #     df=filtered_links_df,\n",
    "    #     score_cols=[\"string_experimental_score\", \"string_textmining_score\", \"string_combined_score\"],\n",
    "    #     quantiles=(0.05, 0.95),\n",
    "    #     apply_log1p=True,\n",
    "    #     sample_frac=0.1  # optional: for speed\n",
    "    # )\n",
    "\n",
    "    filtered_links_df = minmax_normalize_pandas(\n",
    "        df=filtered_links_df,\n",
    "        score_cols=[\"string_experimental_score\", \"string_textmining_score\", \"string_combined_score\"],\n",
    "    )\n",
    "\n",
    "    print(\"  - Merging normalized STRING scores into inferred network\")\n",
    "    merged_dd = inferred_net_df.merge(\n",
    "        filtered_links_df,\n",
    "        left_on=[\"source_id\", \"target_id\"],\n",
    "        right_on=[\"protein1\", \"protein2\"],\n",
    "        how=\"left\"\n",
    "    ).drop(columns=[\"protein1\", \"protein2\"])\n",
    "\n",
    "    print(\"  Done!\")\n",
    "    return merged_dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47900039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inferred_network(inferred_network_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and pivot a melted sparse inferred network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inferred_network_file : str\n",
    "        Path to a parquet file containing a \"melted\" network with columns\n",
    "        ``source_id``, ``peak_id``, ``target_id``, ``score_type`` and\n",
    "        ``score_value``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dask.dataframe.DataFrame\n",
    "        Network in wide format where score types form columns and the index is\n",
    "        ``(source_id, peak_id, target_id)``.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The returned dataframe has a single partition which is usually sufficient\n",
    "    for moderate sized networks.  Adjust ``npartitions`` if necessary.\n",
    "    \"\"\"\n",
    "    print(f\"Loading melted sparse network from: {inferred_network_file}\")\n",
    "    melted_ddf = pd.read_parquet(inferred_network_file, engine=\"pyarrow\")\n",
    "\n",
    "    # Standardize IDs\n",
    "    melted_ddf[\"source_id\"] = melted_ddf[\"source_id\"].str.upper()\n",
    "    melted_ddf[\"target_id\"] = melted_ddf[\"target_id\"].str.upper()\n",
    "\n",
    "    # Aggregate scores\n",
    "    grouped_ddf = (\n",
    "        melted_ddf\n",
    "        .groupby([\"source_id\", \"peak_id\", \"target_id\", \"score_type\"])[\"score_value\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Pivot manually by converting to pandas (if dataset is small enough)\n",
    "    def pivot_partition(df):\n",
    "        return df.pivot_table(\n",
    "            index=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "            columns=\"score_type\",\n",
    "            values=\"score_value\",\n",
    "            aggfunc=\"first\"\n",
    "        ).reset_index()\n",
    "\n",
    "    # Apply pivot in a single partition (best if you've already aggregated)\n",
    "    pivot_df = grouped_ddf  # convert to Pandas here\n",
    "    pivot_df = pivot_partition(pivot_df)\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c717384",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df = read_inferred_network(os.path.join(output_dir, \"inferred_grns\", \"inferred_score_df.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba26993",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df_string = add_string_db_scores(inferred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79602106",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc90ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe to get it in the same format as combine_dataframes.py\n",
    "score_cols = [\n",
    "    'mean_tf_expr', 'mean_peak_expr', 'mean_tg_expr',\n",
    "    'cicero_score', 'TSS_dist_score', 'correlation',\n",
    "    'LITE_score', 'NITE_score', 'chromatin_differential', \n",
    "    'sliding_window_score', 'homer_binding_score', 'string_experimental_score',\n",
    "    'string_textmining_score', 'string_combined_score'\n",
    "    ]\n",
    "\n",
    "melted_df = inferred_df_string.melt(\n",
    "    id_vars=[\"source_id\", \"peak_id\", \"target_id\"],\n",
    "    value_vars=score_cols,\n",
    "    var_name=\"score_type\",\n",
    "    value_name=\"score_value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_df_string.to_parquet(os.path.join(output_dir, \"inferred_grns\", \"inferred_score_df_full.parquet\"), engine=\"pyarrow\", compression=\"snappy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
